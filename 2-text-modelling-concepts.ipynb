{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"markdown","source":"To prepare text data for machine learning, it's essential to convert words into a numerical format. This process, known as integer encoding, assigns a unique integer to each word in the text. The Keras library provides a convenient tool called the Tokenizer API to perform this task.\n\nHere's a breakdown of the process:\n\n1. **Import the Tokenizer:**\n   Start by importing the `Tokenizer` class from the Keras library.\n\n2. **Fit the Tokenizer on Text Data:**\n   Create a `Tokenizer` object and use the `fit_on_texts` method to process your text data. This step associates a unique integer with each word.\n\n3. **Get Integer Encoded Sequences:**\n   Utilize the fitted tokenizer to convert the text data into sequences of integers using the `texts_to_sequences` method. This results in a numerical representation of the original text.\n\n4. **Word Index:**\n   Access the word index, which is a dictionary mapping each word to its corresponding integer. This mapping is useful for understanding the relationship between words and integers.\n\nIn summary, the Tokenizer API in Keras is a powerful tool for transforming text data into a numerical format, enabling the use of machine learning models. The resulting integer sequences serve as input features for natural language processing tasks.","metadata":{}},{"cell_type":"markdown","source":"#  Keras Tokenizer ","metadata":{}},{"cell_type":"markdown","source":"\nThe `Tokenizer` class in Keras is a powerful tool for text preprocessing in natural language processing tasks.\n\n\n```python\nfrom keras.preprocessing.text import Tokenizer\n\n# Create a Tokenizer with specific parameters\ntokenizer = Tokenizer(\n    num_words=5000,  # Limit the number of words to consider based on frequency.\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',  # Define characters to filter out.\n    oov_token=\"<UNK>\"  # Out-of-vocabulary token to represent words not in the vocabulary.\n)\n\n# Fit the Tokenizer on the text data in the \"DocText\" column of the DataFrame\ntokenizer.fit_on_texts(docs_df[\"DocText\"].values)\n```\n\nExplanation of parameters:\n\n- `num_words`: Limits the vocabulary size to the most frequent `num_words` words. Only the most common words will be kept in the vocabulary.\n  \n- `filters`: Specifies a string of characters to filter out from the text. In this case, it includes various punctuation symbols and whitespace characters.\n\n- `oov_token`: Stands for \"out-of-vocabulary\" token. It is a special token used to represent words that are not in the vocabulary.\n\nAfter fitting the tokenizer, you can use it to convert text data to sequences of integers using the `texts_to_sequences` method:\n\n```python\nsequences = tokenizer.texts_to_sequences(docs_df[\"DocText\"].values)\n```\n\nThe resulting `sequences` will contain integer representations of the text data based on the vocabulary learned by the tokenizer.\n\nThis approach is commonly used for preparing text data for input into neural networks or other machine learning models. It helps to represent words in a numerical format suitable for training models on textual data.","metadata":{}},{"cell_type":"markdown","source":"# \"out-of-vocabulary\" token -> UNK\n\nIn natural language processing (NLP), `<UNK>` is a common convention used to represent out-of-vocabulary (OOV) or unknown tokens. When processing text data, a machine learning model might encounter words that were not present in the training data, and these words are considered out-of-vocabulary.\n\nBy setting the `oov_token` parameter to `<UNK>`, you are specifying a token that will be used to represent any word that is not part of the vocabulary learned during training. For example, if the model encounters a word in the test or evaluation data that wasn't present in the training data, it will be replaced with the `<UNK>` token.\n\nHere's an example of how it might be used in practice:\n\n```python\nfrom keras.preprocessing.text import Tokenizer\n\n# Create a Tokenizer with an out-of-vocabulary token\ntokenizer = Tokenizer(oov_token=\"<UNK>\")\n\n# Fit the Tokenizer on training text data\ntexts = [\"apple\", \"banana\", \"orange\"]\ntokenizer.fit_on_texts(texts)\n\n# Convert new text data to sequences, replacing out-of-vocabulary words with <UNK>\nnew_texts = [\"apple\", \"banana\", \"kiwi\"]\nsequences = tokenizer.texts_to_sequences(new_texts)\n\nprint(sequences)\n# Output: [[2], [3], [1]]\n```\n\nIn this example, \"kiwi\" was not present in the training data, so it gets replaced with the `<UNK>` token, which is assigned the index 1. The actual index may vary depending on the specific implementation. The model learns to recognize and handle out-of-vocabulary words during training, improving its generalization to unseen data.","metadata":{}},{"cell_type":"markdown","source":"# num_words\n\nThe `num_words` parameter in the context of Keras' `Tokenizer` is used to limit the vocabulary size by specifying the maximum number of words to keep, based on word frequency. Here's how it works:\n\n- `num_words`: An integer, the maximum number of words to keep in the vocabulary. Only the most frequent `num_words-1` words will be kept, and any less frequent words will be discarded.\n\nIn other words, when you set `num_words=5000`, you are instructing the `Tokenizer` to consider only the top 4999 most frequent words in your dataset, and all other words will be treated as out-of-vocabulary (OOV) words and represented by the OOV token.\n\nHere's an example:\n\n```python\nfrom keras.preprocessing.text import Tokenizer\n\n# Create a Tokenizer with a vocabulary size limit\ntokenizer = Tokenizer(num_words=5000, oov_token=\"<UNK>\")\n\n# Fit the Tokenizer on text data\ntexts = [\"apple\", \"banana\", \"orange\", \"grape\", \"kiwi\", \"mango\", \"banana\"]\ntokenizer.fit_on_texts(texts)\n\n# Convert text data to sequences\nsequences = tokenizer.texts_to_sequences(texts)\n\nprint(sequences)\n# Output: [[2], [3], [1], [4], [1], [1], [3]]\n```\n\nIn this example, the vocabulary is limited to the top 4 most frequent words (excluding OOV token). Words like \"kiwi\" and \"mango\" that are less frequent are replaced with the OOV token. The actual word-to-index mapping may vary depending on the specific implementation details.","metadata":{}},{"cell_type":"markdown","source":"# GloVe (Global Vectors for Word Representation)","metadata":{}},{"cell_type":"markdown","source":"GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations (embeddings) of words. These embeddings capture semantic relationships between words based on the co-occurrence statistics of words in large text corpora.\n\nHere's a general outline of how you can use GloVe vectors in Python:\n\n1. **Download GloVe Vectors:**\n   You need to download pre-trained GloVe vectors. You can find them on the [GloVe website](https://nlp.stanford.edu/projects/glove/) or use other pre-trained models available online.\n\n2. **Load GloVe Vectors:**\n   Once downloaded, you can load GloVe vectors into your Python environment. The vectors are typically stored in a text file, where each line contains a word followed by its vector components.\n\n   Here's a simplified example using Python:\n\n   ```python\n   def load_glove_vectors(file_path):\n       word_vectors = {}\n       with open(file_path, 'r', encoding='utf-8') as file:\n           for line in file:\n               values = line.split()\n               word = values[0]\n               vector = np.array(values[1:], dtype='float32')\n               word_vectors[word] = vector\n       return word_vectors\n\n   # Provide the path to your GloVe file\n   glove_file_path = 'path/to/glove.6B.50d.txt'  # Adjust the file path and dimensions\n   glove_vectors = load_glove_vectors(glove_file_path)\n   ```\n\n   Replace `'path/to/glove.6B.50d.txt'` with the actual path to your GloVe file.\n\n3. **Access Word Vectors:**\n   You can now access the vectors for individual words. For example:\n\n   ```python\n   word_vector = glove_vectors.get('example', None)\n   if word_vector is not None:\n       print(f\"Vector for 'example': {word_vector}\")\n   else:\n       print(\"Word not found in GloVe vectors.\")\n   ```\n\n   Adjust the word (`'example'` in this case) based on your needs.\n\n4. **Utilize GloVe Vectors:**\n   You can use these vectors for various natural language processing tasks, such as word similarity, document classification, sentiment analysis, etc. You can also integrate them into your machine learning models.\n\nKeep in mind that the dimensions of the GloVe vectors depend on the specific model you download (e.g., `glove.6B.50d.txt` corresponds to 50-dimensional vectors, while `glove.6B.300d.txt` corresponds to 300-dimensional vectors). Choose the dimensionality based on your application's requirements.","metadata":{}},{"cell_type":"markdown","source":"# load_glove_vectors","metadata":{}},{"cell_type":"markdown","source":"This code snippet appears to be a Python script for loading GloVe vectors from a file and creating a dictionary (`gvec_index`) to store the word vectors. Here's a breakdown of the code:\n\n```python\n%%time\n\ndef load_glove_vectors(file_path):\n    with open(file_path, encoding=\"utf8\") as txt_f:\n        for line in txt_f:\n            columns = line.split()\n            wrd = columns[0]\n            vec = np.array(columns[1:], dtype=\"float32\")\n            yield wrd, vec\n\n# Provide the path to your GloVe file\nGLOVE_TXT = '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'  # Adjust the file path and dimensions\ngvec_index = dict(load_glove_vectors(GLOVE_TXT))\n```\n\nExplanation:\n\n1. **`%%time`:** This is a Jupyter Notebook magic command that measures the execution time of the code cell.\n\n2. **`load_glove_vectors` function:** This function takes a file path as input (`file_path`) and yields word vectors from the GloVe file. It opens the file, reads each line, splits it into columns, and yields a tuple containing the word (`wrd`) and its corresponding vector (`vec`). The vectors are represented as NumPy arrays of type float32.\n\n3. **`GLOVE_TXT` variable:** This variable holds the file path to the GloVe file. You should adjust the path based on the location of your GloVe file in your Kaggle environment.\n\n4. **`gvec_index` dictionary:** This dictionary is created by calling the `load_glove_vectors` function with the specified GloVe file path. It maps words to their respective vectors, creating a word vector index.\n\nThe code efficiently loads GloVe vectors into memory and creates a dictionary for easy lookup. The `%%time` magic command is used to measure the execution time of this cell.","metadata":{}},{"cell_type":"markdown","source":"#  the word \"the\" and its corresponding GloVe vector","metadata":{}},{"cell_type":"markdown","source":"The provided dictionary entry represents the word \"the\" and its corresponding GloVe vector in the `gvec_index` dictionary. Each word in the GloVe model is represented by a vector of real numbers. Here's a breakdown of the entry:\n\n```python\n'the': array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01, -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04,\n        -6.5660e-01,  2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,  1.1658e-02,  1.0204e-01, -1.2792e-01,\n        -8.4430e-01, -1.2181e-01, -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01, -1.8823e+00, -7.6746e-01,\n         9.9051e-02, -4.2125e-01, -1.9526e-01,  4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,  7.4449e-03,\n         1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02, -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01], dtype=float32)\n```\n\nExplanation:\n\n- The word \"the\" is represented by the key.\n- The value is a NumPy array containing 50 float32 values, which represent the GloVe vector for the word \"the\".\n- Each value in the array corresponds to a specific feature or dimension of the word vector.\n\nThis vector captures the semantic information of the word \"the\" in a continuous vector space, as learned by the GloVe model during training on a large corpus of text. The values in the vector can be interpreted as the word's position in a high-dimensional space, where the distance and direction between vectors reflect semantic relationships between words.","metadata":{}},{"cell_type":"markdown","source":"The values in the GloVe vector represent the word \"the\" in a continuous vector space, capturing various aspects of its semantic meaning. Each value corresponds to a specific feature or dimension in this vector space. Let's break down what these values might represent:\n\nIn the provided GloVe vector for the word \"the\":\n\n```python\n[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01, -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04,\n  -6.5660e-01,  2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,  1.1658e-02,  1.0204e-01, -1.2792e-01,\n  -8.4430e-01, -1.2181e-01, -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01, -1.8823e+00, -7.6746e-01,\n   9.9051e-02, -4.2125e-01, -1.9526e-01,  4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,  7.4449e-03,\n   1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02, -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n   1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]\n```\n\nHere's a general interpretation:\n\n- **Semantic Features:** Each value in the vector represents a semantic feature or characteristic of the word \"the\" in the context of the corpus used to train the GloVe model.\n\n- **Contextual Relationships:** The vector values encode relationships between the word \"the\" and other words. Similar vectors indicate similar contextual usage in the training data.\n\n- **Direction and Magnitude:** The direction and magnitude of the vector convey information about the word's meaning. Words with similar meanings will have vectors that point in similar directions.\n\n- **High-Dimensional Space:** The vector is a point in a high-dimensional space, and distances and angles between vectors provide information about semantic relationships.\n\nIt's important to note that the exact interpretation of each dimension may not be straightforward and is often context-dependent. The power of these vectors lies in their ability to capture complex semantic relationships and context-specific nuances.","metadata":{}},{"cell_type":"markdown","source":"# Why to use GloVe? ","metadata":{}},{"cell_type":"markdown","source":"GloVe, which stands for Global Vectors for Word Representation, is a popular word embedding technique. It is used to represent words as vectors in a continuous vector space where the geometry of the vectors captures semantic relationships between words. Here are some reasons why GloVe is widely used:\n\n1. **Semantic Similarity:** GloVe vectors capture semantic similarity between words. Words with similar meanings or usage patterns are represented by vectors that are closer together in the vector space.\n\n2. **Word Analogies:** GloVe embeddings often exhibit interesting properties, allowing for word analogies to be performed algebraically. For example, the vector for \"king\" - \"man\" + \"woman\" might be close to the vector for \"queen.\"\n\n3. **Pre-trained Models:** GloVe provides pre-trained word vectors on large corpora, which can be beneficial when working with tasks where labeled data is limited. Pre-trained models can be fine-tuned or used as feature representations for downstream tasks.\n\n4. **Generalization:** GloVe vectors are trained on large-scale text data, which enables them to generalize well across various natural language processing (NLP) tasks.\n\n5. **Contextual Information:** GloVe captures contextual information about words based on their co-occurrence statistics in the training corpus. This allows the vectors to encode meaningful relationships between words.\n\n6. **Efficiency:** GloVe vectors are computationally efficient to train and use, making them suitable for a wide range of applications.\n\nIt's important to note that while GloVe is a powerful and widely used technique, there are other word embedding methods like Word2Vec and fastText, each with its strengths and weaknesses. The choice of which method to use often depends on the specific requirements of the task at hand.","metadata":{}},{"cell_type":"markdown","source":"# Word embedding methods ","metadata":{}},{"cell_type":"markdown","source":"Word embedding methods are techniques used in natural language processing (NLP) and machine learning to represent words as dense vectors in a continuous vector space. These methods aim to capture semantic relationships and contextual information about words, enabling machines to understand and process natural language more effectively. Here are some popular word embedding methods:\n\n1. **Word2Vec:**\n   - Developed by Google, Word2Vec is a shallow neural network-based approach that learns word embeddings by predicting the context of words in a given corpus. It includes two models: Continuous Bag of Words (CBOW) and Skip-Gram.\n\n2. **GloVe (Global Vectors for Word Representation):**\n   - GloVe is an unsupervised learning algorithm for obtaining word representations. It focuses on word co-occurrence statistics and constructs a word-word co-occurrence matrix, from which it learns vector representations for words.\n\n3. **FastText:**\n   - Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. This approach allows FastText to capture subword information, making it particularly effective for handling morphologically rich languages and dealing with out-of-vocabulary words.\n\n4. **ELMo (Embeddings from Language Models):**\n   - ELMo uses bidirectional LSTMs (Long Short-Term Memory networks) to generate word embeddings. It captures context-dependent word representations by considering the entire sentence, and the embeddings are dynamic, varying depending on the context in which the word appears.\n\n5. **BERT (Bidirectional Encoder Representations from Transformers):**\n   - Developed by Google, BERT is a transformer-based model that considers the bidirectional context of words. It pre-trains a deep neural network on large amounts of data, and the embeddings it produces are contextualized and highly effective for various downstream NLP tasks.\n\n6. **ULMFiT (Universal Language Model Fine-tuning):**\n   - ULMFiT is a transfer learning approach that pre-trains a language model on a large corpus and then fine-tunes it for specific downstream tasks. This method has been successful in achieving state-of-the-art results in various NLP tasks.\n\n7. **SWEM (Simple Word-Embedding Model):**\n   - SWEM is a simple and efficient word embedding model that averages or pools word embeddings to obtain sentence embeddings. It is computationally less expensive compared to some other methods.\n\nThese word embedding methods play a crucial role in NLP applications such as text classification, sentiment analysis, machine translation, and information retrieval, among others. The choice of the method often depends on the specific task, available data, and computational resources.","metadata":{}}]}