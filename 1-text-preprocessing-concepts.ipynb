{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diaconumadalina/1-text-preprocessing-concepts?scriptVersionId=155272699\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Extract Metadata","metadata":{}},{"cell_type":"markdown","source":"\n\n### 1. Checking for Existing Metadata File:\n\n```python\nif os.path.exists(METADATA_CSV):\n    print(\"Loading metadata from:\", METADATA_CSV)\n    meta_df = pd.read_csv(METADATA_CSV)\n```\n\n- It checks if a CSV file named `METADATA_CSV` (the metadata file path) already exists.\n- If the file exists, it prints a message indicating that metadata is being loaded and reads the CSV file into a Pandas DataFrame (`meta_df`) using `pd.read_csv()`.\n\n### 2. Creating Metadata if File Doesn't Exist:\n\n```python\nelse:\n    meta_data = [\n        [dir_name.capitalize(), f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\", os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)), os.path.join(DOCS_DIR, dir_name, file_name)]\n        for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n        for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n    ]\n\n    col_names = [\"DocType\", \"DocId\", \"FileSize\", \"FilePath\"]\n    meta_df = pd.DataFrame(meta_data, columns=col_names)\n    meta_df.to_csv(METADATA_CSV, index=False, na_rep=\"\")\n    print(\"Metadata saved to:\", METADATA_CSV)\n```\n\n- If the metadata file doesn't exist, it creates a list called `meta_data` using a list comprehension.\n- The list comprehension iterates over directories and files in the specified `DOCS_DIR`. It constructs a list for each file with information such as `DocType`, `DocId`, `FileSize`, and `FilePath`.\n- After collecting metadata, it creates a Pandas DataFrame (`meta_df`) from `meta_data` and saves it to the CSV file using `to_csv()`.\n\n### 3. Changing Data Type of \"DocType\" Column:\n\n```python\nmeta_df[\"DocType\"] = meta_df[\"DocType\"].astype(\"category\")\n```\n\n- Converts the \"DocType\" column in the DataFrame to a categorical data type.\n\n### 4. Displaying a Sample of the DataFrame:\n\n```python\nmeta_df.sample(3)\n```\n\n- Displays a random sample of 3 rows from the DataFrame.\n\nOverall, this code checks if a metadata file exists, loads it if it does, and creates and saves metadata if it doesn't. The metadata includes information about documents in a specified directory, and the resulting DataFrame is modified to use a categorical data type for the \"DocType\" column. Finally, a sample of the DataFrame is displayed.","metadata":{}},{"cell_type":"markdown","source":"# List comprehension","metadata":{}},{"cell_type":"markdown","source":"A list comprehension is a concise way to create lists in Python. It provides a more readable and compact syntax for generating lists compared to traditional for-loops. The basic structure of a list comprehension is as follows:\n\n```python\n[expression for item in iterable if condition]\n```\n\n- **expression:** The expression to be evaluated for each item in the iterable. The result of this expression becomes an element of the new list.\n\n- **item:** The variable representing each element in the iterable (e.g., each item in a list).\n\n- **iterable:** The iterable (e.g., a list, tuple, string, etc.) over which the comprehension is performed.\n\n- **condition (optional):** An optional condition that filters the items. The expression is only evaluated and included in the result if the condition is true.\n\nHere's a simple example to illustrate the concept. Suppose you want to create a list of squares for even numbers from 0 to 9:\n\n```python\nsquares = [x**2 for x in range(10) if x % 2 == 0]\n```\n\nIn this example:\n\n- **expression:** `x**2`\n- **item:** `x`\n- **iterable:** `range(10)`\n- **condition:** `if x % 2 == 0`\n\nThe list comprehension generates a new list `squares` containing the squares of even numbers from 0 to 9. The result is `[0, 4, 16, 36, 64]`. List comprehensions are a powerful and readable way to create lists in a single line of code.","metadata":{}},{"cell_type":"markdown","source":"## `meta_data` list:","metadata":{}},{"cell_type":"markdown","source":"\n```python\nmeta_data = [\n    [\n        dir_name.capitalize(),  # DocType: Capitalized directory name\n        f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # DocId: Capitalized first letter of directory name + underscore + file name without extension\n        os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # FileSize: Size of the file\n        os.path.join(DOCS_DIR, dir_name, file_name)  # FilePath: Full path of the file\n    ]\n    for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n    for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n]\n```\n\nLet's break it down further:\n\n- **Outer Loop:**\n  ```python\n  for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Iterates over the entries in the specified directory (`DOCS_DIR`).\n  - Uses `os.path.isdir()` to filter out non-directory entries.\n  - `os.path.join(DOCS_DIR, dir_name)` is a Python expression using the `os.path.join()` function to construct a path by joining components together. In this specific case:\n\n- `DOCS_DIR`: Represents a directory path.\n- `dir_name`: Represents the name of a subdirectory within the `DOCS_DIR`.\n\n```python\nos.path.join(DOCS_DIR, dir_name)\n```\n\nThe purpose of this expression is to create a full path by joining the directory path (`DOCS_DIR`) and the subdirectory name (`dir_name`). This is commonly used to build paths to files or directories in a platform-independent way.\n\nFor example, if `DOCS_DIR` is something like `\"C:/Documents\"` and `dir_name` is `\"ProjectFiles\"`, the result of `os.path.join(DOCS_DIR, dir_name)` would be `\"C:/Documents/ProjectFiles\"`.\n\nIt's a convenient way to ensure that paths are correctly constructed, taking into account the correct path separator for the operating system (forward slash `/` for Unix-based systems, backslash `\\` for Windows).\n\n\n- **Inner Loop:**\n  ```python\n  for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Nested loop that iterates over the files within each directory.\n\n- **List Elements:**\n  ```python\n  [\n      dir_name.capitalize(),  # Capitalizes the directory name for DocType\n      f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # Creates a unique DocId using the first letter of the directory name, an underscore, and the file name without extension\n      os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # Retrieves the file size\n      os.path.join(DOCS_DIR, dir_name, file_name)  # Constructs the full path of the file\n  ]\n  ```\n  - Creates a list for each file with elements corresponding to `DocType`, `DocId`, `FileSize`, and `FilePath`.\n\nSo, in summary, this list comprehension generates a list of lists, where each inner list represents metadata for a file in the specified directory structure.","metadata":{}},{"cell_type":"markdown","source":"##  Retrieve a list of document ids that appear more than once in the dataset.","metadata":{}},{"cell_type":"markdown","source":"\n```python\nduplicate_doc_ids = [doc_id for doc_id, count in df[\"DocId\"].value_counts().items() if count > 1]\n```\n\n1. **`df[\"DocId\"].value_counts()`:**\n   - `df[\"DocId\"]`: Extracts the \"DocId\" column from the DataFrame `df`.\n   - `.value_counts()`: Counts the occurrences of each unique value in the \"DocId\" column.\n\n2. **`items()`:**\n   - Transforms the result of `value_counts()` into a sequence of (index, count) pairs, where index is a unique document id, and count is the number of occurrences.\n\n3. **List Comprehension:**\n   - `[doc_id for doc_id, count in ... if count > 1]`: Iterates through the (index, count) pairs.\n   - For each pair, it extracts the `doc_id` (unique document id) only if the `count` is greater than 1 (indicating a duplicate).\n   - Creates a list containing the `doc_id` values of duplicate document ids.\n\nIn essence, this line of code creates a list (`duplicate_doc_ids`) containing document ids that have duplicates in the \"DocId\" column of the DataFrame. It leverages the Pandas `value_counts()` function to count occurrences and a list comprehension to filter only the document ids with counts greater than 1.","metadata":{}},{"cell_type":"markdown","source":"## string `\"%1.0f%%\"`","metadata":{}},{"cell_type":"markdown","source":"The format string `\"%1.0f%%\"` is used in Matplotlib's `autopct` parameter to format the percentage display on each wedge of the pie chart. Let's break down the components of this format string:\n\n- **`%`**: The percentage sign is a literal character and will be displayed as is.\n  \n- **`1.0f`**: This part specifies the format for the floating-point number. Here's what each component means:\n  - **`1`**: The minimum width of the entire field, including digits before and after the decimal point.\n  - **`.0`**: The number of digits after the decimal point. In this case, it is set to 0, indicating no decimal places.\n  - **`f`**: The type specifier for the floating-point format.\n\nPutting it all together, `\"%1.0f%%\"` is saying:\n\n- Display the percentage with at least one digit (integer format), and no decimal places, followed by a percentage sign.\n\nThis format is often used when you want to display percentages as whole numbers (e.g., 25% instead of 25.5%). If you want to show a different number of decimal places or include more or fewer digits, you can adjust the format string accordingly.","metadata":{}},{"cell_type":"markdown","source":"## The `enumerate` function ","metadata":{}},{"cell_type":"markdown","source":"The `enumerate` function is used to iterate over a sequence (such as a list) and keep track of the index of the current item. Here's an example:\n\n```python\n# Sample list\nmy_list = ['apple', 'banana', 'orange']\n\n# Using enumerate to get both index and value\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n```\n\nOutput:\n```\nIndex: 0, Value: apple\nIndex: 1, Value: banana\nIndex: 2, Value: orange\n```\n\nIn the context of your code, `enumerate` is likely used to iterate over the unique labels obtained from `pd.factorize(meta_df[\"DocType\"])`. It pairs each unique label with its corresponding encoded value, and `dict(enumerate(...))` creates a dictionary mapping the index (encoded value) to the unique label.\n\nHere's how it might look in your specific case:\n\n```python\n# Sample usage in your code\ncodes, uniques = pd.factorize(meta_df[\"DocType\"])\n\n# Using enumerate to create a mapping from encoded values to original class labels\nclass_label_mapping = dict(enumerate(uniques.categories))\n\n# Display the encoded class labels and their mapping\nprint(\"Encoded class-labels:\\n\", class_label_mapping)\n```\n\nIn this example, `enumerate(uniques.categories)` pairs each unique label with its corresponding index (encoded value), and `dict(enumerate(...))` creates a dictionary for mapping.","metadata":{}},{"cell_type":"markdown","source":"## Elbow","metadata":{}},{"cell_type":"markdown","source":"In other words, identify outliers in the dataset using the Elbow method, a technique that involves detecting data points significantly distant from the overall trend or pattern by examining the point where the rate of change in the data's behavior starts to slow down, forming an \"elbow\" in the analysis.","metadata":{}},{"cell_type":"markdown","source":"## Plot the values of a column within a specified percentile range.","metadata":{}},{"cell_type":"markdown","source":"```python\n\ndef plot_percentile_range(lower_limit, upper_limit, column_name, dataframe, y_label):\n    \"\"\"\n    Plot the values of a column within a specified percentile range.\n\n    Parameters:\n    - lower_limit: float, lower percentile limit\n    - upper_limit: float, upper percentile limit\n    - column_name: str, the name of the column in the DataFrame\n    - dataframe: DataFrame, the input DataFrame\n    - y_label: str, label for the y-axis\n    \"\"\"\n    plt.figure(figsize=(4, 3))\n\n    percentiles = np.arange(lower_limit, upper_limit, 0.01)\n    values = dataframe[column_name].quantile(q=percentiles)\n\n    sns.lineplot(x=percentiles, y=values)\n    plt.title(f\"{y_label} between {lower_limit}% and {round(upper_limit - 0.01, 2)}% percentile\")\n    plt.xlabel(\"Percentile\")\n    plt.ylabel(y_label)\n\n    plt.show()\n\n# Example usage\nplot_percentile_range(10, 90, \"FileSize\", meta_df, \"File size in Bytes\")\n\n```","metadata":{}},{"cell_type":"markdown","source":"The `plot_percentile_range` function looks well-defined and should work for visualizing the values of the \"FileSize\" column within the specified percentile range. The function is designed to generate a line plot using Seaborn, providing insights into how the values of the column vary across the given percentiles.\n\nA couple of notes:\n\n1. **Clarity in Function Name and Parameters:**\n   - The function name `plot_percentile_range` is clear and self-explanatory.\n   - The parameters are well-named and adequately describe their purpose.\n\n2. **Plotting and Visualization:**\n   - The line plot is created using Seaborn's `lineplot`.\n   - The x-axis represents the percentiles, and the y-axis represents the corresponding values of the specified column.\n\n3. **Plot Customization:**\n   - The title, x-axis label, and y-axis label are appropriately customized to provide context to the plot.\n\n4. **Example Usage:**\n   - The example usage at the end demonstrates how to use the function with your DataFrame and the \"FileSize\" column.\n\nOverall, the function appears to be well-written for its intended purpose. If you have any specific questions or if there's anything else you'd like assistance with, feel free to let me know!","metadata":{}},{"cell_type":"markdown","source":"## Percentile","metadata":{}},{"cell_type":"markdown","source":"The line of code `percentiles = np.arange(lower_limit / 100, upper_limit / 100, 0.01)` creates an array of percentiles within a specified range. Let me break down this line:\n\n- `lower_limit / 100`: This expression divides the lower limit by 100 to convert it from a percentage to a decimal.\n\n- `upper_limit / 100`: Similarly, this expression divides the upper limit by 100.\n\n- `np.arange(lower_limit / 100, upper_limit / 100, 0.01)`: This uses NumPy's `arange` function to create an array of values starting from the lower limit (in decimal form), incrementing by 0.01, and stopping just before the upper limit. The result is an array of percentiles ranging from the lower to the upper limit.\n\nFor example, if `lower_limit` is 10 and `upper_limit` is 90, the `percentiles` array will be `array([0.1, 0.11, 0.12, ..., 0.89, 0.9])`.\n\nThis array is then used in the subsequent code to compute the quantiles of the specified column in the DataFrame within this range.","metadata":{}},{"cell_type":"markdown","source":"## Quantile","metadata":{}},{"cell_type":"markdown","source":"The line of code `values = dataframe[column_name].quantile(q=percentiles)` calculates the quantiles of the specified column (`column_name`) in the DataFrame (`dataframe`) for the given percentiles.\n\nHere's what this line does:\n\n- `dataframe[column_name]`: This extracts the values from the specified column in the DataFrame.\n\n- `.quantile(q=percentiles)`: This computes the quantiles of the column values at the specified percentiles. The `q` parameter is set to the array of percentiles created earlier.\n\nThe resulting `values` array contains the quantiles of the column values corresponding to the specified percentiles. Each value in the `values` array represents the data value below which a certain percentage of the data falls.\n\nFor example, if `percentiles` is `array([0.1, 0.2, 0.3])`, then `values` will be an array containing the 10th, 20th, and 30th percentiles of the data in the specified column.\n\n \n Let's take a concrete example with numbers. Consider the following DataFrame:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'FileSize': [100, 150, 200, 250, 300, 350, 400, 450, 500]}\nmeta_df = pd.DataFrame(data)\n```\n\nNow, let's calculate the quantiles for the \"FileSize\" column at specific percentiles:\n\n```python\n# Specify percentiles\npercentiles = [0.1, 0.3, 0.5]\n\n# Calculate quantiles\nvalues = meta_df['FileSize'].quantile(q=percentiles)\n\nprint(values)\n```\n\nOutput:\n```\n0.1    130.0\n0.3    230.0\n0.5    350.0\nName: FileSize, dtype: float64\n```\n\nIn this example, the calculated quantiles are as follows:\n- The 10th percentile (0.1) of the \"FileSize\" column is 130.0.\n- The 30th percentile (0.3) is 230.0.\n- The 50th percentile (0.5), which is the median, is 350.0.\n\nThese values represent the thresholds below which 10%, 30%, and 50% of the data fall in the \"FileSize\" column.","metadata":{}},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"markdown","source":"The value \"130.0\" for the 10th percentile (0.1) is calculated from the \"FileSize\" column in the DataFrame. The `quantile` function in pandas is used to calculate the quantiles.\n\nHere's a step-by-step breakdown of how it's calculated:\n\n1. **Sort the data:** The \"FileSize\" column values are sorted in ascending order.\n\n   ```\n   [100, 150, 200, 250, 300, 350, 400, 450, 500]\n   ```\n\n2. **Identify the position:** The 10th percentile (0.1) corresponds to the position in the sorted list. In this case, it's 10% of the way through the sorted list.\n\n   ```\n   10% of 9 elements = 0.1 * 9 = 0.9\n   ```\n\n   Since the position should be an integer, rounding is applied. Therefore, the 10th percentile is at position 1 in the sorted list.\n\n3. **Retrieve the value:** The value at position 1 in the sorted list is 130.0. Therefore, \"150\" is the 10th percentile value for the \"FileSize\" column.\n\nIn summary, the 10th percentile value is calculated by finding the position in the sorted list corresponding to the specified percentile and retrieving the value at that position.","metadata":{}},{"cell_type":"markdown","source":"# Print list as table","metadata":{}},{"cell_type":"markdown","source":"\n```python\nfor i, (label, percent) in enumerate(table_data, start=1):\n    print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")\n```\n\n1. `for i, (label, percent) in enumerate(table_data, start=1):`\n   - This is a loop that iterates through each row of `table_data` (which contains tuples of class labels and percentages).\n   - `enumerate` is used to get both the index `i` (starting from 1 due to `start=1`) and the tuple `(label, percent)` from each element in `table_data`.\n\n2. `print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")`\n   - This line prints a formatted row of the table for each iteration of the loop.\n   - `f\"| {i:<10} |\"`: Prints the serial number (`i`) left-aligned in a column of width 10.\n   - `{label:<15} |\"`: Prints the class label (`label`) left-aligned in a column of width 15.\n   - `{percent:<24.1f}% |\"`: Prints the percentage (`percent`) left-aligned in a column of width 24 with one decimal place and adds a percentage sign.\n\nThe `:<10`, `:<15`, and `:<24.1f` are examples of string formatting, ensuring that each value is left-aligned within its respective column.\n\nAdjust the widths and formatting as needed based on your preferences and the actual data you are working with.","metadata":{}},{"cell_type":"markdown","source":"# RE.SUB","metadata":{}},{"cell_type":"markdown","source":"\n```python\ntext = re.sub(r\"([^a-z0-9\\s])\\1+\", \" \", text, flags=re.IGNORECASE | re.MULTILINE)\n```\n\nThis regular expression is used to perform substitutions in the `text` variable. Let's break down the components:\n\n1. `r\"([^a-z0-9\\s])\\1+\"`:\n   - `r`: Indicates a raw string, which is used to avoid interpreting backslashes as escape characters.\n   - `([^a-z0-9\\s])`: This is a capturing group that matches any single character that is not a lowercase letter, digit, or whitespace. The parentheses create a capturing group to remember this character.\n   - `\\1+`: This is a backreference to the first capturing group (`\\1`). It matches one or more occurrences of the previously captured character.\n\n2. `\" \"`: The replacement string. It replaces the matched substrings with a single space.\n\n3. `text`: The input text on which the substitution operation is performed.\n\n4. `flags=re.IGNORECASE | re.MULTILINE`: Flags to control the behavior of the regular expression. `re.IGNORECASE` makes the pattern case-insensitive, and `re.MULTILINE` allows the `^` and `$` anchors to match the start/end of each line.\n\nIn summary, this regular expression replaces consecutive occurrences of non-alphanumeric characters (excluding whitespace) with a single space, and it is case-insensitive and works across multiple lines. This is a common approach to clean up and normalize text data.","metadata":{}},{"cell_type":"markdown","source":" The `re.sub(r\"\\s+\", \" \", text).strip()` expression is used for additional cleaning and normalization of the text.\n\n1. `r\"\\s+\"`:\n   - `\\s+`: This regular expression pattern matches one or more occurrences of whitespace characters (including spaces, tabs, and newline characters).\n\n2. `\" \"`:\n   - The replacement string. It replaces consecutive occurrences of whitespace with a single space.\n\n3. `text`:\n   - The input text on which the substitution operation is performed.\n\n4. `.strip()`:\n   - This method is called after the substitution to remove leading and trailing spaces from the resulting string.\n\nIn summary, this expression is used to replace consecutive whitespace characters (including spaces, tabs, and newlines) with a single space and then removes any leading or trailing spaces from the text. It's a common step in text preprocessing to ensure uniform spacing and improve the consistency of the text data.","metadata":{}},{"cell_type":"markdown","source":"# `load_or_parse_text`","metadata":{}},{"cell_type":"markdown","source":" Let's break down the  function step by step:\n\n```python\ndef load_or_parse_text(doc_id, file_path):\n    # Open the file in binary mode (\"rb\")\n    with open(file_path, \"rb\") as txt_f:\n        # Read the contents of the file as bytes and decode them to UTF-8\n        ip_text = txt_f.read().decode(\"utf-8\", errors=\"ignore\").strip()\n\n        # Check if the text is empty or contains only whitespace\n        if not ip_text or ip_text.isspace():\n            # If the text is empty or contains only whitespace, return None\n            return None\n\n        # If the text is not empty, preprocess it using the preprocess function\n        op_text = preprocess(ip_text)\n\n        # Return a list containing document ID, length of the preprocessed text, and the text itself\n        return [doc_id, len(op_text), op_text]\n```\n\nExplanation:\n\n1. **`with open(file_path, \"rb\") as txt_f:`**: This line opens the file specified by `file_path` in binary mode (`\"rb\"`). The `with` statement ensures that the file is properly closed after reading its contents.\n\n2. **`ip_text = txt_f.read().decode(\"utf-8\", errors=\"ignore\").strip()`**: It reads the contents of the file, decodes them to a UTF-8 string, and removes leading and trailing whitespaces using `strip()`. The `errors=\"ignore\"` argument tells Python to ignore any decoding errors that might occur.\n\n3. **`if not ip_text or ip_text.isspace():`**: This condition checks if the decoded text is either empty or consists only of whitespace characters. If true, it means there is no meaningful text, and the function returns `None`.\n\n4. **`op_text = preprocess(ip_text)`**: If the text is not empty, it undergoes preprocessing using the `preprocess` function. The result is stored in `op_text`.\n\n5. **`return [doc_id, len(op_text), op_text]`**: The function returns a list containing the document ID, the length of the preprocessed text, and the preprocessed text itself.\n\nThis function is designed to be part of a larger script or program for processing and analyzing text data. It reads text from a file, checks for emptiness or whitespace, preprocesses the text, and returns relevant information about the document.","metadata":{}},{"cell_type":"markdown","source":"# PARSED_TEXT_CSV","metadata":{}},{"cell_type":"markdown","source":" This code block is responsible for loading or parsing text data from documents and storing it in a DataFrame. Let's break down the code step by step:\n\n```python\n# Check if the parsed text CSV file exists\nif os.path.exists(PARSED_TEXT_CSV):\n    print(\"Loading parsed text data of documents from:\", PARSED_TEXT_CSV)\n    prsd_df = pd.read_csv(PARSED_TEXT_CSV)\nelse:\n    # Use list comprehension to process each document\n    parsed_data = [\n        load_or_parse_text(doc_id, file_path)\n        for doc_id, file_path in tqdm(meta_df[[\"DocId\", \"FilePath\"]].values)\n    ]\n\n    # Filter out None values (documents that were empty after preprocessing)\n    parsed_data = [item for item in parsed_data if item is not None]\n\n    # Convert parsed text from documents into a DataFrame\n    col_names = [\"DocId\", \"DocTextlen\", \"DocText\"]\n    prsd_df = pd.DataFrame(parsed_data, columns=col_names)\n\n    # Save DataFrame as CSV file for future use\n    prsd_df.to_csv(PARSED_TEXT_CSV, index=False, na_rep=\"\")\n    print(\"Parsed text saved to:\", PARSED_TEXT_CSV)\n```\n\nExplanation:\n\n1. **Check if the CSV file exists:**\n   ```python\n   if os.path.exists(PARSED_TEXT_CSV):\n       print(\"Loading parsed text data of documents from:\", PARSED_TEXT_CSV)\n       prsd_df = pd.read_csv(PARSED_TEXT_CSV)\n   ```\n   This part checks if the CSV file (`PARSED_TEXT_CSV`) already exists. If it does, it loads the DataFrame from the CSV file using `pd.read_csv`.\n\n2. **If the CSV file doesn't exist:**\n   ```python\n   else:\n       # Use list comprehension to process each document\n       parsed_data = [\n           load_or_parse_text(doc_id, file_path)\n           for doc_id, file_path in tqdm(meta_df[[\"DocId\", \"FilePath\"]].values)\n       ]\n   ```\n   If the CSV file doesn't exist, it uses a list comprehension to process each document using the `load_or_parse_text` function. It creates a list (`parsed_data`) containing the result for each document.\n\n3. **Filter out None values:**\n   ```python\n   parsed_data = [item for item in parsed_data if item is not None]\n   ```\n   It filters out `None` values from the list, which are returned for documents that were empty or contained only whitespace after preprocessing.\n\n4. **Convert to DataFrame and Save:**\n   ```python\n   col_names = [\"DocId\", \"DocTextlen\", \"DocText\"]\n   prsd_df = pd.DataFrame(parsed_data, columns=col_names)\n\n   # Save DataFrame as CSV file for future use\n   prsd_df.to_csv(PARSED_TEXT_CSV, index=False, na_rep=\"\")\n   print(\"Parsed text saved to:\", PARSED_TEXT_CSV)\n   ```\n   It converts the filtered list (`parsed_data`) into a DataFrame (`prsd_df`) with specified column names. Then, it saves the DataFrame to a CSV file (`PARSED_TEXT_CSV`) for future use.\n\nThis code is part of a data preprocessing pipeline, where text data is loaded or parsed, processed, and saved to a CSV file to avoid repeating the preprocessing steps in the future.","metadata":{}},{"cell_type":"markdown","source":"# strip()","metadata":{}},{"cell_type":"markdown","source":"The `strip()` method in Python is a string method that returns a copy of the string with leading and trailing whitespaces removed. It does not modify the original string; instead, it creates a new string with the leading and trailing whitespaces removed.\n\nHere's a simple example:\n\n```python\noriginal_string = \"   Hello, World!   \"\nstripped_string = original_string.strip()\n\nprint(\"Original String:\", repr(original_string))\nprint(\"Stripped String:\", repr(stripped_string))\n```\n\nOutput:\n```\nOriginal String: '   Hello, World!   '\nStripped String: 'Hello, World!'\n```\n\nIn this example, the `strip()` method removes the leading and trailing whitespaces from the `original_string`.\n\nIt's important to note that \"whitespace\" here includes spaces, tabs, and newline characters. If you want to remove only spaces from the beginning and end of a string, you can use the `lstrip()` method to remove leading spaces and the `rstrip()` method to remove trailing spaces.\n\nHere's an example:\n\n```python\noriginal_string = \"   Hello, World!   \"\nleading_stripped = original_string.lstrip()\ntrailing_stripped = original_string.rstrip()\n\nprint(\"Original String:\", repr(original_string))\nprint(\"Leading Stripped String:\", repr(leading_stripped))\nprint(\"Trailing Stripped String:\", repr(trailing_stripped))\n```\n\nOutput:\n```\nOriginal String: '   Hello, World!   '\nLeading Stripped String: 'Hello, World!   '\nTrailing Stripped String: '   Hello, World!'\n```\n\nIn this example, `lstrip()` removes leading spaces, and `rstrip()` removes trailing spaces.","metadata":{}},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"markdown","source":"In Python, a pipeline typically refers to a series of data processing steps or tasks that are chained together. This is commonly used in various domains, including data science and machine learning. Two popular libraries for creating pipelines in Python are `scikit-learn` for machine learning and `pandas` for data processing. Here, I'll provide a brief overview of how pipelines can be implemented in these contexts:\n\n### 1. **Scikit-Learn for Machine Learning Pipelines:**\n\nScikit-learn provides a `Pipeline` class that allows you to streamline a lot of routine processes, especially in machine learning workflows. Here's a basic example:\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Sample data\n# X, y = ...\n\n# Split the data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a pipeline\nsteps = [\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=3)),\n    ('classifier', SVC())\n]\n\npipeline = Pipeline(steps)\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\nIn this example, the pipeline consists of three steps: scaling the data, reducing dimensionality with PCA, and training a Support Vector Classifier (SVC). The entire pipeline is treated as a single estimator, making it easy to fit, predict, and evaluate.\n\n### 2. **Pandas for Data Processing Pipelines:**\n\nWhen working with data processing tasks using `pandas`, you can create a pipeline using method chaining and the various functions available in the library. Here's a simple example:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\n# df = ...\n\n# Data processing pipeline\nprocessed_data = (\n    df\n    .dropna()  # Remove missing values\n    .drop_duplicates()  # Remove duplicate rows\n    .groupby('category').mean()  # Group by category and calculate the mean\n    .reset_index()  # Reset index\n)\n\n# Display the processed data\nprint(processed_data)\n```\n\nIn this example, the data processing pipeline involves dropping missing values, removing duplicate rows, grouping by a specific column, calculating the mean, and resetting the index.\n\nThese are simplified examples, and actual use cases may involve more complex pipelines with additional steps and parameters. The idea is to create a sequence of operations that can be executed in a structured and reproducible manner.","metadata":{}}]}