{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diaconumadalina/1-text-preprocessing-concepts?scriptVersionId=155058503\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Extract Metadata","metadata":{}},{"cell_type":"markdown","source":"\n\n### 1. Checking for Existing Metadata File:\n\n```python\nif os.path.exists(METADATA_CSV):\n    print(\"Loading metadata from:\", METADATA_CSV)\n    meta_df = pd.read_csv(METADATA_CSV)\n```\n\n- It checks if a CSV file named `METADATA_CSV` (the metadata file path) already exists.\n- If the file exists, it prints a message indicating that metadata is being loaded and reads the CSV file into a Pandas DataFrame (`meta_df`) using `pd.read_csv()`.\n\n### 2. Creating Metadata if File Doesn't Exist:\n\n```python\nelse:\n    meta_data = [\n        [dir_name.capitalize(), f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\", os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)), os.path.join(DOCS_DIR, dir_name, file_name)]\n        for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n        for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n    ]\n\n    col_names = [\"DocType\", \"DocId\", \"FileSize\", \"FilePath\"]\n    meta_df = pd.DataFrame(meta_data, columns=col_names)\n    meta_df.to_csv(METADATA_CSV, index=False, na_rep=\"\")\n    print(\"Metadata saved to:\", METADATA_CSV)\n```\n\n- If the metadata file doesn't exist, it creates a list called `meta_data` using a list comprehension.\n- The list comprehension iterates over directories and files in the specified `DOCS_DIR`. It constructs a list for each file with information such as `DocType`, `DocId`, `FileSize`, and `FilePath`.\n- After collecting metadata, it creates a Pandas DataFrame (`meta_df`) from `meta_data` and saves it to the CSV file using `to_csv()`.\n\n### 3. Changing Data Type of \"DocType\" Column:\n\n```python\nmeta_df[\"DocType\"] = meta_df[\"DocType\"].astype(\"category\")\n```\n\n- Converts the \"DocType\" column in the DataFrame to a categorical data type.\n\n### 4. Displaying a Sample of the DataFrame:\n\n```python\nmeta_df.sample(3)\n```\n\n- Displays a random sample of 3 rows from the DataFrame.\n\nOverall, this code checks if a metadata file exists, loads it if it does, and creates and saves metadata if it doesn't. The metadata includes information about documents in a specified directory, and the resulting DataFrame is modified to use a categorical data type for the \"DocType\" column. Finally, a sample of the DataFrame is displayed.","metadata":{}},{"cell_type":"markdown","source":"# List comprehension","metadata":{}},{"cell_type":"markdown","source":"A list comprehension is a concise way to create lists in Python. It provides a more readable and compact syntax for generating lists compared to traditional for-loops. The basic structure of a list comprehension is as follows:\n\n```python\n[expression for item in iterable if condition]\n```\n\n- **expression:** The expression to be evaluated for each item in the iterable. The result of this expression becomes an element of the new list.\n\n- **item:** The variable representing each element in the iterable (e.g., each item in a list).\n\n- **iterable:** The iterable (e.g., a list, tuple, string, etc.) over which the comprehension is performed.\n\n- **condition (optional):** An optional condition that filters the items. The expression is only evaluated and included in the result if the condition is true.\n\nHere's a simple example to illustrate the concept. Suppose you want to create a list of squares for even numbers from 0 to 9:\n\n```python\nsquares = [x**2 for x in range(10) if x % 2 == 0]\n```\n\nIn this example:\n\n- **expression:** `x**2`\n- **item:** `x`\n- **iterable:** `range(10)`\n- **condition:** `if x % 2 == 0`\n\nThe list comprehension generates a new list `squares` containing the squares of even numbers from 0 to 9. The result is `[0, 4, 16, 36, 64]`. List comprehensions are a powerful and readable way to create lists in a single line of code.","metadata":{}},{"cell_type":"markdown","source":"## `meta_data` list:","metadata":{}},{"cell_type":"markdown","source":"\n```python\nmeta_data = [\n    [\n        dir_name.capitalize(),  # DocType: Capitalized directory name\n        f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # DocId: Capitalized first letter of directory name + underscore + file name without extension\n        os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # FileSize: Size of the file\n        os.path.join(DOCS_DIR, dir_name, file_name)  # FilePath: Full path of the file\n    ]\n    for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n    for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n]\n```\n\nLet's break it down further:\n\n- **Outer Loop:**\n  ```python\n  for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Iterates over the entries in the specified directory (`DOCS_DIR`).\n  - Uses `os.path.isdir()` to filter out non-directory entries.\n  - `os.path.join(DOCS_DIR, dir_name)` is a Python expression using the `os.path.join()` function to construct a path by joining components together. In this specific case:\n\n- `DOCS_DIR`: Represents a directory path.\n- `dir_name`: Represents the name of a subdirectory within the `DOCS_DIR`.\n\n```python\nos.path.join(DOCS_DIR, dir_name)\n```\n\nThe purpose of this expression is to create a full path by joining the directory path (`DOCS_DIR`) and the subdirectory name (`dir_name`). This is commonly used to build paths to files or directories in a platform-independent way.\n\nFor example, if `DOCS_DIR` is something like `\"C:/Documents\"` and `dir_name` is `\"ProjectFiles\"`, the result of `os.path.join(DOCS_DIR, dir_name)` would be `\"C:/Documents/ProjectFiles\"`.\n\nIt's a convenient way to ensure that paths are correctly constructed, taking into account the correct path separator for the operating system (forward slash `/` for Unix-based systems, backslash `\\` for Windows).\n\n\n- **Inner Loop:**\n  ```python\n  for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Nested loop that iterates over the files within each directory.\n\n- **List Elements:**\n  ```python\n  [\n      dir_name.capitalize(),  # Capitalizes the directory name for DocType\n      f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # Creates a unique DocId using the first letter of the directory name, an underscore, and the file name without extension\n      os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # Retrieves the file size\n      os.path.join(DOCS_DIR, dir_name, file_name)  # Constructs the full path of the file\n  ]\n  ```\n  - Creates a list for each file with elements corresponding to `DocType`, `DocId`, `FileSize`, and `FilePath`.\n\nSo, in summary, this list comprehension generates a list of lists, where each inner list represents metadata for a file in the specified directory structure.","metadata":{}},{"cell_type":"markdown","source":"##  Retrieve a list of document ids that appear more than once in the dataset.","metadata":{}},{"cell_type":"markdown","source":"\n```python\nduplicate_doc_ids = [doc_id for doc_id, count in df[\"DocId\"].value_counts().items() if count > 1]\n```\n\n1. **`df[\"DocId\"].value_counts()`:**\n   - `df[\"DocId\"]`: Extracts the \"DocId\" column from the DataFrame `df`.\n   - `.value_counts()`: Counts the occurrences of each unique value in the \"DocId\" column.\n\n2. **`items()`:**\n   - Transforms the result of `value_counts()` into a sequence of (index, count) pairs, where index is a unique document id, and count is the number of occurrences.\n\n3. **List Comprehension:**\n   - `[doc_id for doc_id, count in ... if count > 1]`: Iterates through the (index, count) pairs.\n   - For each pair, it extracts the `doc_id` (unique document id) only if the `count` is greater than 1 (indicating a duplicate).\n   - Creates a list containing the `doc_id` values of duplicate document ids.\n\nIn essence, this line of code creates a list (`duplicate_doc_ids`) containing document ids that have duplicates in the \"DocId\" column of the DataFrame. It leverages the Pandas `value_counts()` function to count occurrences and a list comprehension to filter only the document ids with counts greater than 1.","metadata":{}},{"cell_type":"markdown","source":"## string `\"%1.0f%%\"`","metadata":{}},{"cell_type":"markdown","source":"The format string `\"%1.0f%%\"` is used in Matplotlib's `autopct` parameter to format the percentage display on each wedge of the pie chart. Let's break down the components of this format string:\n\n- **`%`**: The percentage sign is a literal character and will be displayed as is.\n  \n- **`1.0f`**: This part specifies the format for the floating-point number. Here's what each component means:\n  - **`1`**: The minimum width of the entire field, including digits before and after the decimal point.\n  - **`.0`**: The number of digits after the decimal point. In this case, it is set to 0, indicating no decimal places.\n  - **`f`**: The type specifier for the floating-point format.\n\nPutting it all together, `\"%1.0f%%\"` is saying:\n\n- Display the percentage with at least one digit (integer format), and no decimal places, followed by a percentage sign.\n\nThis format is often used when you want to display percentages as whole numbers (e.g., 25% instead of 25.5%). If you want to show a different number of decimal places or include more or fewer digits, you can adjust the format string accordingly.","metadata":{}},{"cell_type":"markdown","source":"## The `enumerate` function ","metadata":{}},{"cell_type":"markdown","source":"The `enumerate` function is used to iterate over a sequence (such as a list) and keep track of the index of the current item. Here's an example:\n\n```python\n# Sample list\nmy_list = ['apple', 'banana', 'orange']\n\n# Using enumerate to get both index and value\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n```\n\nOutput:\n```\nIndex: 0, Value: apple\nIndex: 1, Value: banana\nIndex: 2, Value: orange\n```\n\nIn the context of your code, `enumerate` is likely used to iterate over the unique labels obtained from `pd.factorize(meta_df[\"DocType\"])`. It pairs each unique label with its corresponding encoded value, and `dict(enumerate(...))` creates a dictionary mapping the index (encoded value) to the unique label.\n\nHere's how it might look in your specific case:\n\n```python\n# Sample usage in your code\ncodes, uniques = pd.factorize(meta_df[\"DocType\"])\n\n# Using enumerate to create a mapping from encoded values to original class labels\nclass_label_mapping = dict(enumerate(uniques.categories))\n\n# Display the encoded class labels and their mapping\nprint(\"Encoded class-labels:\\n\", class_label_mapping)\n```\n\nIn this example, `enumerate(uniques.categories)` pairs each unique label with its corresponding index (encoded value), and `dict(enumerate(...))` creates a dictionary for mapping.","metadata":{}},{"cell_type":"markdown","source":"## Elbow","metadata":{}},{"cell_type":"markdown","source":"In other words, identify outliers in the dataset using the Elbow method, a technique that involves detecting data points significantly distant from the overall trend or pattern by examining the point where the rate of change in the data's behavior starts to slow down, forming an \"elbow\" in the analysis.","metadata":{}},{"cell_type":"markdown","source":"## Plot the values of a column within a specified percentile range.","metadata":{}},{"cell_type":"markdown","source":"```python\n\ndef plot_percentile_range(lower_limit, upper_limit, column_name, dataframe, y_label):\n    \"\"\"\n    Plot the values of a column within a specified percentile range.\n\n    Parameters:\n    - lower_limit: float, lower percentile limit\n    - upper_limit: float, upper percentile limit\n    - column_name: str, the name of the column in the DataFrame\n    - dataframe: DataFrame, the input DataFrame\n    - y_label: str, label for the y-axis\n    \"\"\"\n    plt.figure(figsize=(4, 3))\n\n    percentiles = np.arange(lower_limit, upper_limit, 0.01)\n    values = dataframe[column_name].quantile(q=percentiles)\n\n    sns.lineplot(x=percentiles, y=values)\n    plt.title(f\"{y_label} between {lower_limit}% and {round(upper_limit - 0.01, 2)}% percentile\")\n    plt.xlabel(\"Percentile\")\n    plt.ylabel(y_label)\n\n    plt.show()\n\n# Example usage\nplot_percentile_range(10, 90, \"FileSize\", meta_df, \"File size in Bytes\")\n\n```","metadata":{}},{"cell_type":"markdown","source":"The `plot_percentile_range` function looks well-defined and should work for visualizing the values of the \"FileSize\" column within the specified percentile range. The function is designed to generate a line plot using Seaborn, providing insights into how the values of the column vary across the given percentiles.\n\nA couple of notes:\n\n1. **Clarity in Function Name and Parameters:**\n   - The function name `plot_percentile_range` is clear and self-explanatory.\n   - The parameters are well-named and adequately describe their purpose.\n\n2. **Plotting and Visualization:**\n   - The line plot is created using Seaborn's `lineplot`.\n   - The x-axis represents the percentiles, and the y-axis represents the corresponding values of the specified column.\n\n3. **Plot Customization:**\n   - The title, x-axis label, and y-axis label are appropriately customized to provide context to the plot.\n\n4. **Example Usage:**\n   - The example usage at the end demonstrates how to use the function with your DataFrame and the \"FileSize\" column.\n\nOverall, the function appears to be well-written for its intended purpose. If you have any specific questions or if there's anything else you'd like assistance with, feel free to let me know!","metadata":{}},{"cell_type":"markdown","source":"## Percentile","metadata":{}},{"cell_type":"markdown","source":"The line of code `percentiles = np.arange(lower_limit / 100, upper_limit / 100, 0.01)` creates an array of percentiles within a specified range. Let me break down this line:\n\n- `lower_limit / 100`: This expression divides the lower limit by 100 to convert it from a percentage to a decimal.\n\n- `upper_limit / 100`: Similarly, this expression divides the upper limit by 100.\n\n- `np.arange(lower_limit / 100, upper_limit / 100, 0.01)`: This uses NumPy's `arange` function to create an array of values starting from the lower limit (in decimal form), incrementing by 0.01, and stopping just before the upper limit. The result is an array of percentiles ranging from the lower to the upper limit.\n\nFor example, if `lower_limit` is 10 and `upper_limit` is 90, the `percentiles` array will be `array([0.1, 0.11, 0.12, ..., 0.89, 0.9])`.\n\nThis array is then used in the subsequent code to compute the quantiles of the specified column in the DataFrame within this range.","metadata":{}},{"cell_type":"markdown","source":"## Quantile","metadata":{}},{"cell_type":"markdown","source":"The line of code `values = dataframe[column_name].quantile(q=percentiles)` calculates the quantiles of the specified column (`column_name`) in the DataFrame (`dataframe`) for the given percentiles.\n\nHere's what this line does:\n\n- `dataframe[column_name]`: This extracts the values from the specified column in the DataFrame.\n\n- `.quantile(q=percentiles)`: This computes the quantiles of the column values at the specified percentiles. The `q` parameter is set to the array of percentiles created earlier.\n\nThe resulting `values` array contains the quantiles of the column values corresponding to the specified percentiles. Each value in the `values` array represents the data value below which a certain percentage of the data falls.\n\nFor example, if `percentiles` is `array([0.1, 0.2, 0.3])`, then `values` will be an array containing the 10th, 20th, and 30th percentiles of the data in the specified column.\n\n \n Let's take a concrete example with numbers. Consider the following DataFrame:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'FileSize': [100, 150, 200, 250, 300, 350, 400, 450, 500]}\nmeta_df = pd.DataFrame(data)\n```\n\nNow, let's calculate the quantiles for the \"FileSize\" column at specific percentiles:\n\n```python\n# Specify percentiles\npercentiles = [0.1, 0.3, 0.5]\n\n# Calculate quantiles\nvalues = meta_df['FileSize'].quantile(q=percentiles)\n\nprint(values)\n```\n\nOutput:\n```\n0.1    130.0\n0.3    230.0\n0.5    350.0\nName: FileSize, dtype: float64\n```\n\nIn this example, the calculated quantiles are as follows:\n- The 10th percentile (0.1) of the \"FileSize\" column is 130.0.\n- The 30th percentile (0.3) is 230.0.\n- The 50th percentile (0.5), which is the median, is 350.0.\n\nThese values represent the thresholds below which 10%, 30%, and 50% of the data fall in the \"FileSize\" column.","metadata":{}},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"markdown","source":"The value \"130.0\" for the 10th percentile (0.1) is calculated from the \"FileSize\" column in the DataFrame. The `quantile` function in pandas is used to calculate the quantiles.\n\nHere's a step-by-step breakdown of how it's calculated:\n\n1. **Sort the data:** The \"FileSize\" column values are sorted in ascending order.\n\n   ```\n   [100, 150, 200, 250, 300, 350, 400, 450, 500]\n   ```\n\n2. **Identify the position:** The 10th percentile (0.1) corresponds to the position in the sorted list. In this case, it's 10% of the way through the sorted list.\n\n   ```\n   10% of 9 elements = 0.1 * 9 = 0.9\n   ```\n\n   Since the position should be an integer, rounding is applied. Therefore, the 10th percentile is at position 1 in the sorted list.\n\n3. **Retrieve the value:** The value at position 1 in the sorted list is 130.0. Therefore, \"150\" is the 10th percentile value for the \"FileSize\" column.\n\nIn summary, the 10th percentile value is calculated by finding the position in the sorted list corresponding to the specified percentile and retrieving the value at that position.","metadata":{}},{"cell_type":"markdown","source":"# Print list as table","metadata":{}},{"cell_type":"markdown","source":"\n```python\nfor i, (label, percent) in enumerate(table_data, start=1):\n    print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")\n```\n\n1. `for i, (label, percent) in enumerate(table_data, start=1):`\n   - This is a loop that iterates through each row of `table_data` (which contains tuples of class labels and percentages).\n   - `enumerate` is used to get both the index `i` (starting from 1 due to `start=1`) and the tuple `(label, percent)` from each element in `table_data`.\n\n2. `print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")`\n   - This line prints a formatted row of the table for each iteration of the loop.\n   - `f\"| {i:<10} |\"`: Prints the serial number (`i`) left-aligned in a column of width 10.\n   - `{label:<15} |\"`: Prints the class label (`label`) left-aligned in a column of width 15.\n   - `{percent:<24.1f}% |\"`: Prints the percentage (`percent`) left-aligned in a column of width 24 with one decimal place and adds a percentage sign.\n\nThe `:<10`, `:<15`, and `:<24.1f` are examples of string formatting, ensuring that each value is left-aligned within its respective column.\n\nAdjust the widths and formatting as needed based on your preferences and the actual data you are working with.","metadata":{}}]}