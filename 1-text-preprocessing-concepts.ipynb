{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/diaconumadalina/1-text-preprocessing-concepts?scriptVersionId=156856677\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Extract Metadata","metadata":{}},{"cell_type":"markdown","source":"\n\n### 1. Checking for Existing Metadata File:\n\n```python\nif os.path.exists(METADATA_CSV):\n    print(\"Loading metadata from:\", METADATA_CSV)\n    meta_df = pd.read_csv(METADATA_CSV)\n```\n\n- It checks if a CSV file named `METADATA_CSV` (the metadata file path) already exists.\n- If the file exists, it prints a message indicating that metadata is being loaded and reads the CSV file into a Pandas DataFrame (`meta_df`) using `pd.read_csv()`.\n\n### 2. Creating Metadata if File Doesn't Exist:\n\n```python\nelse:\n    meta_data = [\n        [dir_name.capitalize(), f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\", os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)), os.path.join(DOCS_DIR, dir_name, file_name)]\n        for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n        for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n    ]\n\n    col_names = [\"DocType\", \"DocId\", \"FileSize\", \"FilePath\"]\n    meta_df = pd.DataFrame(meta_data, columns=col_names)\n    meta_df.to_csv(METADATA_CSV, index=False, na_rep=\"\")\n    print(\"Metadata saved to:\", METADATA_CSV)\n```\n\n- If the metadata file doesn't exist, it creates a list called `meta_data` using a list comprehension.\n- The list comprehension iterates over directories and files in the specified `DOCS_DIR`. It constructs a list for each file with information such as `DocType`, `DocId`, `FileSize`, and `FilePath`.\n- After collecting metadata, it creates a Pandas DataFrame (`meta_df`) from `meta_data` and saves it to the CSV file using `to_csv()`.\n\n### 3. Changing Data Type of \"DocType\" Column:\n\n```python\nmeta_df[\"DocType\"] = meta_df[\"DocType\"].astype(\"category\")\n```\n\n- Converts the \"DocType\" column in the DataFrame to a categorical data type.\n\n### 4. Displaying a Sample of the DataFrame:\n\n```python\nmeta_df.sample(3)\n```\n\n- Displays a random sample of 3 rows from the DataFrame.\n\nOverall, this code checks if a metadata file exists, loads it if it does, and creates and saves metadata if it doesn't. The metadata includes information about documents in a specified directory, and the resulting DataFrame is modified to use a categorical data type for the \"DocType\" column. Finally, a sample of the DataFrame is displayed.","metadata":{}},{"cell_type":"markdown","source":"# List comprehension","metadata":{}},{"cell_type":"markdown","source":"A list comprehension is a concise way to create lists in Python. It provides a more readable and compact syntax for generating lists compared to traditional for-loops. The basic structure of a list comprehension is as follows:\n\n```python\n[expression for item in iterable if condition]\n```\n\n- **expression:** The expression to be evaluated for each item in the iterable. The result of this expression becomes an element of the new list.\n\n- **item:** The variable representing each element in the iterable (e.g., each item in a list).\n\n- **iterable:** The iterable (e.g., a list, tuple, string, etc.) over which the comprehension is performed.\n\n- **condition (optional):** An optional condition that filters the items. The expression is only evaluated and included in the result if the condition is true.\n\nHere's a simple example to illustrate the concept. Suppose you want to create a list of squares for even numbers from 0 to 9:\n\n```python\nsquares = [x**2 for x in range(10) if x % 2 == 0]\n```\n\nIn this example:\n\n- **expression:** `x**2`\n- **item:** `x`\n- **iterable:** `range(10)`\n- **condition:** `if x % 2 == 0`\n\nThe list comprehension generates a new list `squares` containing the squares of even numbers from 0 to 9. The result is `[0, 4, 16, 36, 64]`. List comprehensions are a powerful and readable way to create lists in a single line of code.","metadata":{}},{"cell_type":"markdown","source":"## `meta_data` list:","metadata":{}},{"cell_type":"markdown","source":"\n```python\nmeta_data = [\n    [\n        dir_name.capitalize(),  # DocType: Capitalized directory name\n        f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # DocId: Capitalized first letter of directory name + underscore + file name without extension\n        os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # FileSize: Size of the file\n        os.path.join(DOCS_DIR, dir_name, file_name)  # FilePath: Full path of the file\n    ]\n    for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n    for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n]\n```\n\nLet's break it down further:\n\n- **Outer Loop:**\n  ```python\n  for dir_name in os.listdir(DOCS_DIR) if os.path.isdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Iterates over the entries in the specified directory (`DOCS_DIR`).\n  - Uses `os.path.isdir()` to filter out non-directory entries.\n  - `os.path.join(DOCS_DIR, dir_name)` is a Python expression using the `os.path.join()` function to construct a path by joining components together. In this specific case:\n\n- `DOCS_DIR`: Represents a directory path.\n- `dir_name`: Represents the name of a subdirectory within the `DOCS_DIR`.\n\n```python\nos.path.join(DOCS_DIR, dir_name)\n```\n\nThe purpose of this expression is to create a full path by joining the directory path (`DOCS_DIR`) and the subdirectory name (`dir_name`). This is commonly used to build paths to files or directories in a platform-independent way.\n\nFor example, if `DOCS_DIR` is something like `\"C:/Documents\"` and `dir_name` is `\"ProjectFiles\"`, the result of `os.path.join(DOCS_DIR, dir_name)` would be `\"C:/Documents/ProjectFiles\"`.\n\nIt's a convenient way to ensure that paths are correctly constructed, taking into account the correct path separator for the operating system (forward slash `/` for Unix-based systems, backslash `\\` for Windows).\n\n\n- **Inner Loop:**\n  ```python\n  for file_name in os.listdir(os.path.join(DOCS_DIR, dir_name))\n  ```\n  - Nested loop that iterates over the files within each directory.\n\n- **List Elements:**\n  ```python\n  [\n      dir_name.capitalize(),  # Capitalizes the directory name for DocType\n      f\"{dir_name[0].upper()}_{os.path.splitext(file_name)[0]}\",  # Creates a unique DocId using the first letter of the directory name, an underscore, and the file name without extension\n      os.path.getsize(os.path.join(DOCS_DIR, dir_name, file_name)),  # Retrieves the file size\n      os.path.join(DOCS_DIR, dir_name, file_name)  # Constructs the full path of the file\n  ]\n  ```\n  - Creates a list for each file with elements corresponding to `DocType`, `DocId`, `FileSize`, and `FilePath`.\n\nSo, in summary, this list comprehension generates a list of lists, where each inner list represents metadata for a file in the specified directory structure.","metadata":{}},{"cell_type":"markdown","source":"##  Retrieve a list of document ids that appear more than once in the dataset.","metadata":{}},{"cell_type":"markdown","source":"\n```python\nduplicate_doc_ids = [doc_id for doc_id, count in df[\"DocId\"].value_counts().items() if count > 1]\n```\n\n1. **`df[\"DocId\"].value_counts()`:**\n   - `df[\"DocId\"]`: Extracts the \"DocId\" column from the DataFrame `df`.\n   - `.value_counts()`: Counts the occurrences of each unique value in the \"DocId\" column.\n\n2. **`items()`:**\n   - Transforms the result of `value_counts()` into a sequence of (index, count) pairs, where index is a unique document id, and count is the number of occurrences.\n\n3. **List Comprehension:**\n   - `[doc_id for doc_id, count in ... if count > 1]`: Iterates through the (index, count) pairs.\n   - For each pair, it extracts the `doc_id` (unique document id) only if the `count` is greater than 1 (indicating a duplicate).\n   - Creates a list containing the `doc_id` values of duplicate document ids.\n\nIn essence, this line of code creates a list (`duplicate_doc_ids`) containing document ids that have duplicates in the \"DocId\" column of the DataFrame. It leverages the Pandas `value_counts()` function to count occurrences and a list comprehension to filter only the document ids with counts greater than 1.","metadata":{}},{"cell_type":"markdown","source":"## string `\"%1.0f%%\"`","metadata":{}},{"cell_type":"markdown","source":"The format string `\"%1.0f%%\"` is used in Matplotlib's `autopct` parameter to format the percentage display on each wedge of the pie chart. Let's break down the components of this format string:\n\n- **`%`**: The percentage sign is a literal character and will be displayed as is.\n  \n- **`1.0f`**: This part specifies the format for the floating-point number. Here's what each component means:\n  - **`1`**: The minimum width of the entire field, including digits before and after the decimal point.\n  - **`.0`**: The number of digits after the decimal point. In this case, it is set to 0, indicating no decimal places.\n  - **`f`**: The type specifier for the floating-point format.\n\nPutting it all together, `\"%1.0f%%\"` is saying:\n\n- Display the percentage with at least one digit (integer format), and no decimal places, followed by a percentage sign.\n\nThis format is often used when you want to display percentages as whole numbers (e.g., 25% instead of 25.5%). If you want to show a different number of decimal places or include more or fewer digits, you can adjust the format string accordingly.","metadata":{}},{"cell_type":"markdown","source":"## The `enumerate` function ","metadata":{}},{"cell_type":"markdown","source":"The `enumerate` function is used to iterate over a sequence (such as a list) and keep track of the index of the current item. Here's an example:\n\n```python\n# Sample list\nmy_list = ['apple', 'banana', 'orange']\n\n# Using enumerate to get both index and value\nfor index, value in enumerate(my_list):\n    print(f\"Index: {index}, Value: {value}\")\n```\n\nOutput:\n```\nIndex: 0, Value: apple\nIndex: 1, Value: banana\nIndex: 2, Value: orange\n```\n\nIn the context of your code, `enumerate` is likely used to iterate over the unique labels obtained from `pd.factorize(meta_df[\"DocType\"])`. It pairs each unique label with its corresponding encoded value, and `dict(enumerate(...))` creates a dictionary mapping the index (encoded value) to the unique label.\n\nHere's how it might look in your specific case:\n\n```python\n# Sample usage in your code\ncodes, uniques = pd.factorize(meta_df[\"DocType\"])\n\n# Using enumerate to create a mapping from encoded values to original class labels\nclass_label_mapping = dict(enumerate(uniques.categories))\n\n# Display the encoded class labels and their mapping\nprint(\"Encoded class-labels:\\n\", class_label_mapping)\n```\n\nIn this example, `enumerate(uniques.categories)` pairs each unique label with its corresponding index (encoded value), and `dict(enumerate(...))` creates a dictionary for mapping.","metadata":{}},{"cell_type":"markdown","source":"## Elbow","metadata":{}},{"cell_type":"markdown","source":"In other words, identify outliers in the dataset using the Elbow method, a technique that involves detecting data points significantly distant from the overall trend or pattern by examining the point where the rate of change in the data's behavior starts to slow down, forming an \"elbow\" in the analysis.","metadata":{}},{"cell_type":"markdown","source":"## Plot the values of a column within a specified percentile range.","metadata":{}},{"cell_type":"markdown","source":"```python\n\ndef plot_percentile_range(lower_limit, upper_limit, column_name, dataframe, y_label):\n    \"\"\"\n    Plot the values of a column within a specified percentile range.\n\n    Parameters:\n    - lower_limit: float, lower percentile limit\n    - upper_limit: float, upper percentile limit\n    - column_name: str, the name of the column in the DataFrame\n    - dataframe: DataFrame, the input DataFrame\n    - y_label: str, label for the y-axis\n    \"\"\"\n    plt.figure(figsize=(4, 3))\n\n    percentiles = np.arange(lower_limit, upper_limit, 0.01)\n    values = dataframe[column_name].quantile(q=percentiles)\n\n    sns.lineplot(x=percentiles, y=values)\n    plt.title(f\"{y_label} between {lower_limit}% and {round(upper_limit - 0.01, 2)}% percentile\")\n    plt.xlabel(\"Percentile\")\n    plt.ylabel(y_label)\n\n    plt.show()\n\n# Example usage\nplot_percentile_range(10, 90, \"FileSize\", meta_df, \"File size in Bytes\")\n\n```","metadata":{}},{"cell_type":"markdown","source":"The `plot_percentile_range` function looks well-defined and should work for visualizing the values of the \"FileSize\" column within the specified percentile range. The function is designed to generate a line plot using Seaborn, providing insights into how the values of the column vary across the given percentiles.\n\nA couple of notes:\n\n1. **Clarity in Function Name and Parameters:**\n   - The function name `plot_percentile_range` is clear and self-explanatory.\n   - The parameters are well-named and adequately describe their purpose.\n\n2. **Plotting and Visualization:**\n   - The line plot is created using Seaborn's `lineplot`.\n   - The x-axis represents the percentiles, and the y-axis represents the corresponding values of the specified column.\n\n3. **Plot Customization:**\n   - The title, x-axis label, and y-axis label are appropriately customized to provide context to the plot.\n\n4. **Example Usage:**\n   - The example usage at the end demonstrates how to use the function with your DataFrame and the \"FileSize\" column.\n\nOverall, the function appears to be well-written for its intended purpose. If you have any specific questions or if there's anything else you'd like assistance with, feel free to let me know!","metadata":{}},{"cell_type":"markdown","source":"## Percentile","metadata":{}},{"cell_type":"markdown","source":"The line of code `percentiles = np.arange(lower_limit / 100, upper_limit / 100, 0.01)` creates an array of percentiles within a specified range. Let me break down this line:\n\n- `lower_limit / 100`: This expression divides the lower limit by 100 to convert it from a percentage to a decimal.\n\n- `upper_limit / 100`: Similarly, this expression divides the upper limit by 100.\n\n- `np.arange(lower_limit / 100, upper_limit / 100, 0.01)`: This uses NumPy's `arange` function to create an array of values starting from the lower limit (in decimal form), incrementing by 0.01, and stopping just before the upper limit. The result is an array of percentiles ranging from the lower to the upper limit.\n\nFor example, if `lower_limit` is 10 and `upper_limit` is 90, the `percentiles` array will be `array([0.1, 0.11, 0.12, ..., 0.89, 0.9])`.\n\nThis array is then used in the subsequent code to compute the quantiles of the specified column in the DataFrame within this range.","metadata":{}},{"cell_type":"markdown","source":"## Quantile","metadata":{}},{"cell_type":"markdown","source":"The line of code `values = dataframe[column_name].quantile(q=percentiles)` calculates the quantiles of the specified column (`column_name`) in the DataFrame (`dataframe`) for the given percentiles.\n\nHere's what this line does:\n\n- `dataframe[column_name]`: This extracts the values from the specified column in the DataFrame.\n\n- `.quantile(q=percentiles)`: This computes the quantiles of the column values at the specified percentiles. The `q` parameter is set to the array of percentiles created earlier.\n\nThe resulting `values` array contains the quantiles of the column values corresponding to the specified percentiles. Each value in the `values` array represents the data value below which a certain percentage of the data falls.\n\nFor example, if `percentiles` is `array([0.1, 0.2, 0.3])`, then `values` will be an array containing the 10th, 20th, and 30th percentiles of the data in the specified column.\n\n \n Let's take a concrete example with numbers. Consider the following DataFrame:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\ndata = {'FileSize': [100, 150, 200, 250, 300, 350, 400, 450, 500]}\nmeta_df = pd.DataFrame(data)\n```\n\nNow, let's calculate the quantiles for the \"FileSize\" column at specific percentiles:\n\n```python\n# Specify percentiles\npercentiles = [0.1, 0.3, 0.5]\n\n# Calculate quantiles\nvalues = meta_df['FileSize'].quantile(q=percentiles)\n\nprint(values)\n```\n\nOutput:\n```\n0.1    130.0\n0.3    230.0\n0.5    350.0\nName: FileSize, dtype: float64\n```\n\nIn this example, the calculated quantiles are as follows:\n- The 10th percentile (0.1) of the \"FileSize\" column is 130.0.\n- The 30th percentile (0.3) is 230.0.\n- The 50th percentile (0.5), which is the median, is 350.0.\n\nThese values represent the thresholds below which 10%, 30%, and 50% of the data fall in the \"FileSize\" column.","metadata":{}},{"cell_type":"markdown","source":"## Example","metadata":{}},{"cell_type":"markdown","source":"The value \"130.0\" for the 10th percentile (0.1) is calculated from the \"FileSize\" column in the DataFrame. The `quantile` function in pandas is used to calculate the quantiles.\n\nHere's a step-by-step breakdown of how it's calculated:\n\n1. **Sort the data:** The \"FileSize\" column values are sorted in ascending order.\n\n   ```\n   [100, 150, 200, 250, 300, 350, 400, 450, 500]\n   ```\n\n2. **Identify the position:** The 10th percentile (0.1) corresponds to the position in the sorted list. In this case, it's 10% of the way through the sorted list.\n\n   ```\n   10% of 9 elements = 0.1 * 9 = 0.9\n   ```\n\n   Since the position should be an integer, rounding is applied. Therefore, the 10th percentile is at position 1 in the sorted list.\n\n3. **Retrieve the value:** The value at position 1 in the sorted list is 130.0. Therefore, \"150\" is the 10th percentile value for the \"FileSize\" column.\n\nIn summary, the 10th percentile value is calculated by finding the position in the sorted list corresponding to the specified percentile and retrieving the value at that position.","metadata":{}},{"cell_type":"markdown","source":"# Print list as table","metadata":{}},{"cell_type":"markdown","source":"\n```python\nfor i, (label, percent) in enumerate(table_data, start=1):\n    print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")\n```\n\n1. `for i, (label, percent) in enumerate(table_data, start=1):`\n   - This is a loop that iterates through each row of `table_data` (which contains tuples of class labels and percentages).\n   - `enumerate` is used to get both the index `i` (starting from 1 due to `start=1`) and the tuple `(label, percent)` from each element in `table_data`.\n\n2. `print(f\"| {i:<10} | {label:<15} | {percent:<24}% |\")`\n   - This line prints a formatted row of the table for each iteration of the loop.\n   - `f\"| {i:<10} |\"`: Prints the serial number (`i`) left-aligned in a column of width 10.\n   - `{label:<15} |\"`: Prints the class label (`label`) left-aligned in a column of width 15.\n   - `{percent:<24.1f}% |\"`: Prints the percentage (`percent`) left-aligned in a column of width 24 with one decimal place and adds a percentage sign.\n\nThe `:<10`, `:<15`, and `:<24.1f` are examples of string formatting, ensuring that each value is left-aligned within its respective column.\n\nAdjust the widths and formatting as needed based on your preferences and the actual data you are working with.","metadata":{}},{"cell_type":"markdown","source":"# RE.SUB","metadata":{}},{"cell_type":"markdown","source":"\n```python\ntext = re.sub(r\"([^a-z0-9\\s])\\1+\", \" \", text, flags=re.IGNORECASE | re.MULTILINE)\n```\n\nThis regular expression is used to perform substitutions in the `text` variable. Let's break down the components:\n\n1. `r\"([^a-z0-9\\s])\\1+\"`:\n   - `r`: Indicates a raw string, which is used to avoid interpreting backslashes as escape characters.\n   - `([^a-z0-9\\s])`: This is a capturing group that matches any single character that is not a lowercase letter, digit, or whitespace. The parentheses create a capturing group to remember this character.\n   - `\\1+`: This is a backreference to the first capturing group (`\\1`). It matches one or more occurrences of the previously captured character.\n\n2. `\" \"`: The replacement string. It replaces the matched substrings with a single space.\n\n3. `text`: The input text on which the substitution operation is performed.\n\n4. `flags=re.IGNORECASE | re.MULTILINE`: Flags to control the behavior of the regular expression. `re.IGNORECASE` makes the pattern case-insensitive, and `re.MULTILINE` allows the `^` and `$` anchors to match the start/end of each line.\n\nIn summary, this regular expression replaces consecutive occurrences of non-alphanumeric characters (excluding whitespace) with a single space, and it is case-insensitive and works across multiple lines. This is a common approach to clean up and normalize text data.","metadata":{}},{"cell_type":"markdown","source":" The `re.sub(r\"\\s+\", \" \", text).strip()` expression is used for additional cleaning and normalization of the text.\n\n1. `r\"\\s+\"`:\n   - `\\s+`: This regular expression pattern matches one or more occurrences of whitespace characters (including spaces, tabs, and newline characters).\n\n2. `\" \"`:\n   - The replacement string. It replaces consecutive occurrences of whitespace with a single space.\n\n3. `text`:\n   - The input text on which the substitution operation is performed.\n\n4. `.strip()`:\n   - This method is called after the substitution to remove leading and trailing spaces from the resulting string.\n\nIn summary, this expression is used to replace consecutive whitespace characters (including spaces, tabs, and newlines) with a single space and then removes any leading or trailing spaces from the text. It's a common step in text preprocessing to ensure uniform spacing and improve the consistency of the text data.","metadata":{}},{"cell_type":"markdown","source":"# `load_or_parse_text`","metadata":{}},{"cell_type":"markdown","source":" Let's break down the  function step by step:\n\n```python\ndef load_or_parse_text(doc_id, file_path):\n    # Open the file in binary mode (\"rb\")\n    with open(file_path, \"rb\") as txt_f:\n        # Read the contents of the file as bytes and decode them to UTF-8\n        ip_text = txt_f.read().decode(\"utf-8\", errors=\"ignore\").strip()\n\n        # Check if the text is empty or contains only whitespace\n        if not ip_text or ip_text.isspace():\n            # If the text is empty or contains only whitespace, return None\n            return None\n\n        # If the text is not empty, preprocess it using the preprocess function\n        op_text = preprocess(ip_text)\n\n        # Return a list containing document ID, length of the preprocessed text, and the text itself\n        return [doc_id, len(op_text), op_text]\n```\n\nExplanation:\n\n1. **`with open(file_path, \"rb\") as txt_f:`**: This line opens the file specified by `file_path` in binary mode (`\"rb\"`). The `with` statement ensures that the file is properly closed after reading its contents.\n\n2. **`ip_text = txt_f.read().decode(\"utf-8\", errors=\"ignore\").strip()`**: It reads the contents of the file, decodes them to a UTF-8 string, and removes leading and trailing whitespaces using `strip()`. The `errors=\"ignore\"` argument tells Python to ignore any decoding errors that might occur.\n\n3. **`if not ip_text or ip_text.isspace():`**: This condition checks if the decoded text is either empty or consists only of whitespace characters. If true, it means there is no meaningful text, and the function returns `None`.\n\n4. **`op_text = preprocess(ip_text)`**: If the text is not empty, it undergoes preprocessing using the `preprocess` function. The result is stored in `op_text`.\n\n5. **`return [doc_id, len(op_text), op_text]`**: The function returns a list containing the document ID, the length of the preprocessed text, and the preprocessed text itself.\n\nThis function is designed to be part of a larger script or program for processing and analyzing text data. It reads text from a file, checks for emptiness or whitespace, preprocesses the text, and returns relevant information about the document.","metadata":{}},{"cell_type":"markdown","source":"# PARSED_TEXT_CSV","metadata":{}},{"cell_type":"markdown","source":" This code block is responsible for loading or parsing text data from documents and storing it in a DataFrame. Let's break down the code step by step:\n\n```python\n# Check if the parsed text CSV file exists\nif os.path.exists(PARSED_TEXT_CSV):\n    print(\"Loading parsed text data of documents from:\", PARSED_TEXT_CSV)\n    prsd_df = pd.read_csv(PARSED_TEXT_CSV)\nelse:\n    # Use list comprehension to process each document\n    parsed_data = [\n        load_or_parse_text(doc_id, file_path)\n        for doc_id, file_path in tqdm(meta_df[[\"DocId\", \"FilePath\"]].values)\n    ]\n\n    # Filter out None values (documents that were empty after preprocessing)\n    parsed_data = [item for item in parsed_data if item is not None]\n\n    # Convert parsed text from documents into a DataFrame\n    col_names = [\"DocId\", \"DocTextlen\", \"DocText\"]\n    prsd_df = pd.DataFrame(parsed_data, columns=col_names)\n\n    # Save DataFrame as CSV file for future use\n    prsd_df.to_csv(PARSED_TEXT_CSV, index=False, na_rep=\"\")\n    print(\"Parsed text saved to:\", PARSED_TEXT_CSV)\n```\n\nExplanation:\n\n1. **Check if the CSV file exists:**\n   ```python\n   if os.path.exists(PARSED_TEXT_CSV):\n       print(\"Loading parsed text data of documents from:\", PARSED_TEXT_CSV)\n       prsd_df = pd.read_csv(PARSED_TEXT_CSV)\n   ```\n   This part checks if the CSV file (`PARSED_TEXT_CSV`) already exists. If it does, it loads the DataFrame from the CSV file using `pd.read_csv`.\n\n2. **If the CSV file doesn't exist:**\n   ```python\n   else:\n       # Use list comprehension to process each document\n       parsed_data = [\n           load_or_parse_text(doc_id, file_path)\n           for doc_id, file_path in tqdm(meta_df[[\"DocId\", \"FilePath\"]].values)\n       ]\n   ```\n   If the CSV file doesn't exist, it uses a list comprehension to process each document using the `load_or_parse_text` function. It creates a list (`parsed_data`) containing the result for each document.\n\n3. **Filter out None values:**\n   ```python\n   parsed_data = [item for item in parsed_data if item is not None]\n   ```\n   It filters out `None` values from the list, which are returned for documents that were empty or contained only whitespace after preprocessing.\n\n4. **Convert to DataFrame and Save:**\n   ```python\n   col_names = [\"DocId\", \"DocTextlen\", \"DocText\"]\n   prsd_df = pd.DataFrame(parsed_data, columns=col_names)\n\n   # Save DataFrame as CSV file for future use\n   prsd_df.to_csv(PARSED_TEXT_CSV, index=False, na_rep=\"\")\n   print(\"Parsed text saved to:\", PARSED_TEXT_CSV)\n   ```\n   It converts the filtered list (`parsed_data`) into a DataFrame (`prsd_df`) with specified column names. Then, it saves the DataFrame to a CSV file (`PARSED_TEXT_CSV`) for future use.\n\nThis code is part of a data preprocessing pipeline, where text data is loaded or parsed, processed, and saved to a CSV file to avoid repeating the preprocessing steps in the future.","metadata":{}},{"cell_type":"markdown","source":"# strip()","metadata":{}},{"cell_type":"markdown","source":"The `strip()` method in Python is a string method that returns a copy of the string with leading and trailing whitespaces removed. It does not modify the original string; instead, it creates a new string with the leading and trailing whitespaces removed.\n\nHere's a simple example:\n\n```python\noriginal_string = \"   Hello, World!   \"\nstripped_string = original_string.strip()\n\nprint(\"Original String:\", repr(original_string))\nprint(\"Stripped String:\", repr(stripped_string))\n```\n\nOutput:\n```\nOriginal String: '   Hello, World!   '\nStripped String: 'Hello, World!'\n```\n\nIn this example, the `strip()` method removes the leading and trailing whitespaces from the `original_string`.\n\nIt's important to note that \"whitespace\" here includes spaces, tabs, and newline characters. If you want to remove only spaces from the beginning and end of a string, you can use the `lstrip()` method to remove leading spaces and the `rstrip()` method to remove trailing spaces.\n\nHere's an example:\n\n```python\noriginal_string = \"   Hello, World!   \"\nleading_stripped = original_string.lstrip()\ntrailing_stripped = original_string.rstrip()\n\nprint(\"Original String:\", repr(original_string))\nprint(\"Leading Stripped String:\", repr(leading_stripped))\nprint(\"Trailing Stripped String:\", repr(trailing_stripped))\n```\n\nOutput:\n```\nOriginal String: '   Hello, World!   '\nLeading Stripped String: 'Hello, World!   '\nTrailing Stripped String: '   Hello, World!'\n```\n\nIn this example, `lstrip()` removes leading spaces, and `rstrip()` removes trailing spaces.","metadata":{}},{"cell_type":"markdown","source":"# Pipeline","metadata":{}},{"cell_type":"markdown","source":"In Python, a pipeline typically refers to a series of data processing steps or tasks that are chained together. This is commonly used in various domains, including data science and machine learning. Two popular libraries for creating pipelines in Python are `scikit-learn` for machine learning and `pandas` for data processing. Here, I'll provide a brief overview of how pipelines can be implemented in these contexts:\n\n### 1. **Scikit-Learn for Machine Learning Pipelines:**\n\nScikit-learn provides a `Pipeline` class that allows you to streamline a lot of routine processes, especially in machine learning workflows. Here's a basic example:\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Sample data\n# X, y = ...\n\n# Split the data into training and testing sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a pipeline\nsteps = [\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=3)),\n    ('classifier', SVC())\n]\n\npipeline = Pipeline(steps)\n\n# Fit the pipeline on the training data\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy}\")\n```\n\nIn this example, the pipeline consists of three steps: scaling the data, reducing dimensionality with PCA, and training a Support Vector Classifier (SVC). The entire pipeline is treated as a single estimator, making it easy to fit, predict, and evaluate.\n\n### 2. **Pandas for Data Processing Pipelines:**\n\nWhen working with data processing tasks using `pandas`, you can create a pipeline using method chaining and the various functions available in the library. Here's a simple example:\n\n```python\nimport pandas as pd\n\n# Sample DataFrame\n# df = ...\n\n# Data processing pipeline\nprocessed_data = (\n    df\n    .dropna()  # Remove missing values\n    .drop_duplicates()  # Remove duplicate rows\n    .groupby('category').mean()  # Group by category and calculate the mean\n    .reset_index()  # Reset index\n)\n\n# Display the processed data\nprint(processed_data)\n```\n\nIn this example, the data processing pipeline involves dropping missing values, removing duplicate rows, grouping by a specific column, calculating the mean, and resetting the index.\n\nThese are simplified examples, and actual use cases may involve more complex pipelines with additional steps and parameters. The idea is to create a sequence of operations that can be executed in a structured and reproducible manner.","metadata":{}},{"cell_type":"markdown","source":"# NLP","metadata":{}},{"cell_type":"markdown","source":"NLP stands for Natural Language Processing, which is a subfield of artificial intelligence (AI) that focuses on the interaction between computers and humans through natural language. The ultimate objective of NLP is to enable computers to understand, interpret, and generate human language in a way that is both meaningful and contextually relevant.\n\nKey tasks and components within NLP include:\n\n1. **Tokenization:**\n   - Breaking down a text into individual words or \"tokens.\" This is a fundamental step in many NLP processes.\n\n2. **Part-of-Speech Tagging:**\n   - Assigning grammatical parts of speech (such as nouns, verbs, adjectives) to each word in a sentence.\n\n3. **Named Entity Recognition (NER):**\n   - Identifying and classifying entities (e.g., person names, locations, organizations) in a text.\n\n4. **Sentiment Analysis:**\n   - Determining the sentiment or emotion expressed in a piece of text, often categorized as positive, negative, or neutral.\n\n5. **Text Classification:**\n   - Assigning predefined categories or labels to a document or piece of text based on its content.\n\n6. **Language Modeling:**\n   - Building statistical models or neural networks that capture the structure and patterns of a language, enabling tasks like text generation.\n\n7. **Machine Translation:**\n   - Translating text from one language to another automatically using computational methods.\n\n8. **Coreference Resolution:**\n   - Identifying when two or more words or phrases in a text refer to the same entity.\n\n9. **Information Extraction:**\n   - Extracting structured information from unstructured text, such as finding relationships between entities.\n\n10. **Speech Recognition:**\n    - Converting spoken language into written text.\n\nNLP has a wide range of applications, including virtual assistants, chatbots, language translation services, sentiment analysis for social media monitoring, and information retrieval. Advances in machine learning and deep learning, as well as the availability of large datasets, have significantly contributed to the progress of NLP in recent years. Popular NLP libraries and frameworks include spaCy, NLTK, and Transformers (using libraries like Hugging Face's Transformers).","metadata":{}},{"cell_type":"markdown","source":"# Function to process language","metadata":{}},{"cell_type":"markdown","source":"```python\ndef decontract_text(doc, patterns):\n    # Special cases of decontraction.\n    atrib_rlr = nlp.get_pipe(\"attribute_ruler\")\n    atrib_rlr.add_patterns(patterns)\n\n    return doc\n\ndef lemmatize_and_filter(doc, rm_stp=True, rm_pun=True):\n    lemmas = [tkn.lemma_.strip().lower() for tkn in doc if not (rm_stp and tkn.is_stop) and not (rm_pun and tkn.is_punct)]\n    return \" \".join(lemmas)\n\ndef capture_pos_count(doc, Id=None):\n    pos_count = {\"DocId\": Id} if Id else {}\n    for item in sorted(doc.count_by(spacy.attrs.POS).items(), key=lambda i: i[0]):\n        pos_count[doc.vocab[item[0]].text] = item[1]\n\n    return pos_count\n\ndef process_language(Id, text, rm_stp=True, rm_pun=True, decontraction_patterns=None):\n    \"\"\"\n    Function to process language by\n        1. Removing stop-words and punctuation marks.\n        2. Lemmatizing tokens.\n        3. Chunking entities.\n    \"\"\"\n    doc = nlp(str(text))\n\n    if decontraction_patterns:\n        doc = decontract_text(doc, decontraction_patterns)\n\n    lmtzd_txt = lemmatize_and_filter(doc, rm_stp, rm_pun)\n    pos_count = capture_pos_count(doc, Id)\n\n    return lmtzd_txt, pos_count\n```","metadata":{}},{"cell_type":"markdown","source":"Your code looks well-structured and modular. It separates different processing steps into distinct functions, making the overall logic clearer and more maintainable. However, since the `decontract_text` function is currently a placeholder that doesn't perform any specific decontraction, you might want to customize it based on your specific requirements.\n\nHere's a quick summary of the functions:\n\n- `decontract_text`: Intended for handling decontraction based on specified patterns, but currently, it just adds patterns without actually modifying the document. Make sure to update it to suit your decontraction needs.\n\n- `lemmatize_and_filter`: Performs lemmatization and filtering of stop words and punctuation. It uses list comprehension for concise code.\n\n- `capture_pos_count`: Captures the part-of-speech (POS) count in the document, allowing you to analyze the distribution of POS tags.\n\n- `process_language`: Combines the above functions to process a piece of text. It takes parameters for controlling whether to remove stop words and punctuation, as well as an optional list of decontraction patterns.\n\nIf you have specific decontraction patterns or additional requirements for text processing, you can further customize the `decontract_text` function. Otherwise, your code structure seems well-organized and ready for integration into a larger natural language processing (NLP) pipeline. If you have any specific questions or if there's anything else you'd like assistance with, feel free to ask!","metadata":{}},{"cell_type":"markdown","source":"# Used spaCy to process a text document (`doc`) and create a list of lemmatized tokens. \n","metadata":{}},{"cell_type":"markdown","source":"\n```python\nlemmas = [\n    tkn.lemma_.strip().lower()  # Get the lemmatized form of the token, remove leading/trailing whitespaces, and convert to lowercase\n    for tkn in doc  # Iterate over each token in the processed document\n    if not (rm_stp and tkn.is_stop)  # If remove stop words is True and the token is a stop word, skip the token\n    and not (rm_pun and tkn.is_punct)  # If remove punctuation is True and the token is punctuation, skip the token\n]\n```\n\nHere's a breakdown of the components:\n\n- `tkn.lemma_.strip().lower()`: This extracts the lemmatized form of the token, removes leading and trailing whitespaces, and converts the result to lowercase.\n- `for tkn in doc`: This iterates over each token in the spaCy processed document.\n- `if not (rm_stp and tkn.is_stop)`: If the `rm_stp` (remove stop words) variable is True and the token is a stop word, it is skipped.\n- `and not (rm_pun and tkn.is_punct)`: If the `rm_pun` (remove punctuation) variable is True and the token is punctuation, it is skipped.\n\nIn summary, this code creates a list of lemmatized tokens from a spaCy processed document, and it can optionally remove stop words and/or punctuation based on the values of the `rm_stp` and `rm_pun` variables.","metadata":{}},{"cell_type":"markdown","source":"# spaCy and adding patterns to the Attribute Ruler component. \n","metadata":{}},{"cell_type":"markdown","source":"\n```python\natrib_rlr = nlp.get_pipe(\"attribute_ruler\")\natrib_rlr.add_patterns(patterns)\n```\n\n1. `nlp.get_pipe(\"attribute_ruler\")`: This retrieves the Attribute Ruler component from the spaCy pipeline (`nlp`).\n\n2. `atrib_rlr.add_patterns(patterns)`: This adds patterns to the Attribute Ruler. The `patterns` variable should contain the patterns you want to add. Patterns are typically rules or configurations that the Attribute Ruler uses to match and annotate tokens in the processed text.\n\nIn summary, these two lines of code retrieve the Attribute Ruler from the spaCy pipeline and then add custom patterns to it. The patterns you provide will influence how the Attribute Ruler processes and annotates tokens during text analysis.","metadata":{}},{"cell_type":"markdown","source":"# the Attribute Ruler","metadata":{}},{"cell_type":"markdown","source":"The Attribute Ruler is a component in spaCy's pipeline that is responsible for adding token attributes based on custom rules or patterns. It allows you to define patterns that match tokens in the processed text and then assign attributes to those tokens or modify existing attributes. This can be useful for adding domain-specific information, customizing the tokenization process, or enhancing the information available in the token annotations.\n\nHere are some key points about the Attribute Ruler component:\n\n1. **Pattern Matching:** The Attribute Ruler uses patterns to match tokens in the text. These patterns can be based on various criteria, such as token text, part-of-speech tags, or other linguistic features.\n\n2. **Attribute Assignment:** Once a pattern is matched, the Attribute Ruler can assign custom attributes to the corresponding tokens or modify existing attributes. This allows you to enrich the token annotations with domain-specific information.\n\n3. **Customization:** Users can define their own patterns and rules to tailor the behavior of the Attribute Ruler to specific requirements. This makes it a versatile tool for handling various linguistic or domain-specific tasks.\n\nHere's a simple example of how you might use the Attribute Ruler to add a custom attribute to tokens matching a specific pattern:\n\n```python\n# Example patterns\npatterns = [\n    {\"LOWER\": \"apple\", \"POS\": \"NOUN\"},\n    {\"LOWER\": \"orange\", \"POS\": \"NOUN\"}\n]\n\n# Adding patterns to the Attribute Ruler\natrib_rlr = nlp.get_pipe(\"attribute_ruler\")\natrib_rlr.add_patterns(patterns)\n```\n\nIn this example, the patterns specify tokens with the lowercase text \"apple\" or \"orange\" and a part-of-speech tag of \"NOUN.\" When these patterns are added to the Attribute Ruler, tokens matching these criteria will be annotated with a custom attribute or modified in some way, as defined by the rules associated with the patterns.","metadata":{}},{"cell_type":"markdown","source":"# counting the occurrences of different parts of speech (POS)","metadata":{}},{"cell_type":"markdown","source":" counting the occurrences of different parts of speech (POS) in a spaCy `Doc` object and storing the counts in a dictionary called `pos_count`. Here's a breakdown of the code:\n\n```python\npos_count = {\"DocId\": Id} if Id else {}\n```\n\nThis line initializes `pos_count` with a dictionary containing a key \"DocId\" and its corresponding value `Id` if `Id` is truthy. If `Id` is falsy, an empty dictionary `{}` is assigned to `pos_count`.\n\n```python\nfor item in sorted(doc.count_by(spacy.attrs.POS).items(), key=lambda i: i[0]):\n    pos_count[doc.vocab[item[0]].text] = item[1]\n```\n\nThis loop iterates over the items returned by `doc.count_by(spacy.attrs.POS)`, which provides a count of occurrences for each part of speech in the document. The loop sorts these items based on the part of speech (using `key=lambda i: i[0]`), and then it assigns the part-of-speech text (retrieved from `doc.vocab[item[0]].text`) and its corresponding count to the `pos_count` dictionary.\n\nIn summary, after this loop, `pos_count` will be a dictionary where keys are part-of-speech labels, and values are the counts of occurrences of each part of speech in the document. The `Id` (if provided) is associated with the key \"DocId\" in this dictionary.","metadata":{}},{"cell_type":"markdown","source":"The `key=lambda i: i[0]` expression is used as an argument to the `sorted` function in the provided code snippet. Let's break down what this lambda function does:\n\n```python\nkey=lambda i: i[0]\n```\n\n- `lambda i: i[0]`: This defines an anonymous (lambda) function that takes an argument `i` and returns its first element (`i[0]`).\n\nThe `sorted` function is then using this lambda function as the key to determine the sorting order. In this specific case, `sorted` is sorting the items based on their first element (`i[0]`), which is typically used when you want to sort a list of tuples or key-value pairs based on the keys.\n\nSo, in the context of the provided code:\n\n```python\nfor item in sorted(doc.count_by(spacy.attrs.POS).items(), key=lambda i: i[0]):\n    # Rest of the loop\n```\n\nThe items returned by `doc.count_by(spacy.attrs.POS).items()` are sorted based on their first element (the POS values), and the loop processes them in this sorted order. This can be useful when you want to iterate over the POS counts in a specific order, such as alphabetical order by the POS labels.","metadata":{}},{"cell_type":"markdown","source":"In natural language processing (NLP), \"POS\" typically refers to \"Part of Speech.\" Part-of-speech tagging is a process in which words in a text are assigned a grammatical category (part of speech), such as noun, verb, adjective, adverb, etc. These categories provide information about the syntactic and grammatical role of each word in a sentence.\n\nHere are some common POS tags:\n\n- **Noun (N):** A word that represents a person, place, thing, or idea.\n- **Verb (V):** A word that describes an action or occurrence.\n- **Adjective (ADJ):** A word that describes or modifies a noun.\n- **Adverb (ADV):** A word that describes or modifies a verb, adjective, or another adverb.\n- **Pronoun (PRON):** A word that takes the place of a noun (e.g., he, she, it).\n- **Preposition (ADP):** A word that shows the relationship between a noun (or pronoun) and other words in a sentence.\n- **Conjunction (CONJ):** A word that connects words, phrases, or clauses.\n- **Interjection (INTJ):** A word or phrase expressing strong emotion or surprise.\n\nIn spaCy, POS tags are represented as integer values. When you see `doc.vocab[item[0]].text` in the provided code snippet, it is used to convert the integer POS label to its corresponding text representation.\n\nFor example, if `item[0]` is `84`, then `doc.vocab[item[0]].text` might be `'NOUN'` because spaCy uses numerical representations for efficiency but provides a mapping to human-readable text for easier interpretation.","metadata":{}},{"cell_type":"markdown","source":"In the provided code snippet:\n\n```python\npos_count[doc.vocab[item[0]].text] = item[1]\n```\n\n`item[1]` corresponds to the count of occurrences of a specific part-of-speech (POS) label in the document. Let's break down the components:\n\n- `item[0]`: This is the first element of the tuple `item`, representing a POS label. It's an integer value.\n\n- `doc.vocab[item[0]].text`: This retrieves the text representation of the POS label using spaCy's vocabulary. It converts the integer POS label to its corresponding textual representation (e.g., 'NOUN', 'VERB', etc.).\n\n- `item[1]`: This is the second element of the tuple `item`, representing the count of occurrences of the corresponding POS label in the document.\n\nThe line `pos_count[doc.vocab[item[0]].text] = item[1]` is assigning the count (`item[1]`) to the dictionary `pos_count` with the key being the textual representation of the POS label (`doc.vocab[item[0]].text`). This way, you end up with a dictionary (`pos_count`) where the keys are POS labels (in human-readable text form), and the values are the counts of occurrences of each part of speech in the document.","metadata":{}},{"cell_type":"markdown","source":"Let's consider a simple example to illustrate how the `capture_pos_count` function works. Assume you have a spaCy `Doc` object representing the following sentence:\n\n```python\nimport spacy\n\n# Load the spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Process a sample sentence\nsample_sentence = \"The quick brown fox jumps over the lazy dog.\"\ndoc = nlp(sample_sentence)\n\n# Call the capture_pos_count function\npos_counts = capture_pos_count(doc)\n\n# Display the result\nprint(pos_counts)\n```\n\nThis would output something like:\n\n```python\n{'DocId': None, 'ADJ': 1, 'DET': 2, 'NOUN': 3, 'PART': 1, 'PRON': 1, 'VERB': 1}\n```\n\nExplanation:\n\n- The `Doc` object contains the processed information for the sentence.\n- The `capture_pos_count` function is called with the `doc` object.\n- The function counts the occurrences of each part-of-speech (POS) label in the document.\n- The result is a dictionary (`pos_counts`) where keys are POS labels, and values are the counts of occurrences.\n- In this example, the sentence contains one adjective ('quick'), two determiners ('The', 'the'), three nouns ('fox', 'dog', 'jumps'), one particle ('over'), one pronoun ('lazy'), and one verb ('jumps').\n- The 'DocId' key is included in the dictionary with a value of `None` because no specific 'Id' parameter was provided in this example.\n","metadata":{}},{"cell_type":"markdown","source":"# process_language function","metadata":{}},{"cell_type":"markdown","source":"It looks like the `process_language` function is designed to process language by performing several tasks, including removing stop-words and punctuation marks, lemmatizing tokens, chunking entities, and capturing part-of-speech counts. Here's a brief breakdown of the function:\n\n```python\ndef process_language(Id, text, rm_stp=True, rm_pun=True, decontraction_patterns=None):\n    \"\"\"\n    Function to process language by\n        1. Removing stop-words and punctuation marks.\n        2. Lemmatizing tokens.\n        3. Chunking entities.\n    \"\"\"\n    # Process the input text with spaCy\n    doc = nlp(str(text))\n\n    # Apply decontraction patterns if provided\n    if decontraction_patterns:\n        doc = decontract_text(doc, decontraction_patterns)\n\n    # Lemmatize tokens and capture part-of-speech counts\n    lmtzd_txt = lemmatize_and_filter(doc, rm_stp, rm_pun)\n    pos_count = capture_pos_count(doc, Id)\n\n    # Return the lemmatized text and part-of-speech counts\n    return lmtzd_txt, pos_count\n```\n\nThis function takes the following parameters:\n\n- `Id`: Identifier associated with the processed text.\n- `text`: Input text to be processed.\n- `rm_stp`: Boolean flag indicating whether to remove stop-words (default is True).\n- `rm_pun`: Boolean flag indicating whether to remove punctuation marks (default is True).\n- `decontraction_patterns`: Optional decontraction patterns to handle contractions in the text.\n\nHere's a summary of what the function does:\n\n1. **Text Processing:** It processes the input text using spaCy, creating a spaCy `Doc` object.\n2. **Decontraction:** If decontraction patterns are provided, it applies them to the processed text.\n3. **Lemmatization and Filtering:** It lemmatizes the tokens in the document and filters out stop-words and punctuation based on the specified flags (`rm_stp` and `rm_pun`).\n4. **Part-of-Speech Counts:** It captures the part-of-speech counts using the `capture_pos_count` function.\n\nThe function then returns the lemmatized text and the part-of-speech counts.","metadata":{}},{"cell_type":"markdown","source":"# The line `corr_mat.values[np.eye(corr_mat.shape[0], dtype=bool)] = 0` ","metadata":{}},{"cell_type":"markdown","source":"The line `corr_mat.values[np.eye(corr_mat.shape[0], dtype=bool)] = 0` is using NumPy's boolean indexing and the `np.eye` function to set the diagonal elements of the correlation matrix (`corr_mat`) to zero.\n\nHere's a breakdown of how this line works:\n\n1. `np.eye(corr_mat.shape[0], dtype=bool)`: This creates a boolean identity matrix of size `corr_mat.shape[0]` (number of rows in the correlation matrix). The `dtype=bool` ensures that the elements of the identity matrix are boolean.\n\n2. `corr_mat.values[np.eye(corr_mat.shape[0], dtype=bool)]`: This uses boolean indexing to select elements in the correlation matrix where the corresponding element in the boolean identity matrix is `True`. In this case, it selects only the diagonal elements of the correlation matrix.\n\n3. `= 0`: Finally, it assigns the value `0` to the selected diagonal elements, effectively resetting the variance of each variable to zero.\n\nSo, in summary, this line is a concise way to set the diagonal elements of the correlation matrix to zero, ensuring that the variance of each variable is reset.","metadata":{}},{"cell_type":"markdown","source":"### Example","metadata":{}},{"cell_type":"markdown","source":"Let's consider a simple example to illustrate the concept using a 3x3 correlation matrix. Suppose we have the following correlation matrix:\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# Sample correlation matrix\ndata = {\n    'A': [1.0, 0.8, 0.2],\n    'B': [0.8, 1.0, 0.5],\n    'C': [0.2, 0.5, 1.0]\n}\n\ndf = pd.DataFrame(data)\ncorr_mat = df.corr()\n\nprint(\"Original Correlation Matrix:\")\nprint(corr_mat)\n```\n\nThis will give us the original correlation matrix:\n\n```\n   A    B    C\nA  1.0  0.8  0.2\nB  0.8  1.0  0.5\nC  0.2  0.5  1.0\n```\n\nNow, let's use the line `corr_mat.values[np.eye(corr_mat.shape[0], dtype=bool)] = 0` to set the diagonal elements to zero:\n\n```python\n# Set diagonal elements to zero\ncorr_mat.values[np.eye(corr_mat.shape[0], dtype=bool)] = 0\n\nprint(\"\\nCorrelation Matrix with Variance Reset to Zero:\")\nprint(corr_mat)\n```\n\nThe modified correlation matrix will be:\n\n```\n   A    B    C\nA  0.0  0.8  0.2\nB  0.8  0.0  0.5\nC  0.2  0.5  0.0\n```\n\nNow, the diagonal elements are set to zero, which represents the variance of each variable. This ensures that the diagonal elements (variance) are reset to zero while keeping the off-diagonal elements unchanged.","metadata":{}},{"cell_type":"markdown","source":"### Text extraction and Cleanup","metadata":{}},{"cell_type":"markdown","source":"This Python code defines a function called `preprocess_v2` that takes a text input and performs several text preprocessing steps. Let's break down each part of the code:\n\n1. `normalized_text = unicodedata.normalize('NFKD', text)`: This line normalizes Unicode characters using the NFKD (Normalization Form Compatibility Decomposition) form. It helps in handling different representations of characters and making them more consistent.\n\n2. `cleaned_text = re.sub(r\"([^a-z0-9\\s])\\1+\", \" \", normalized_text)`: This line uses the `re.sub` function from the `re` module to perform a regular expression-based substitution. It replaces consecutive occurrences of non-alphanumeric and non-whitespace characters (except specified punctuation) with a single space. The pattern `([^a-z0-9\\s])\\1+` is explained in a previous response, and it captures and replaces repeated occurrences of the same character.\n\n3. `cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()`: This line uses another `re.sub` to replace consecutive whitespace characters with a single space. The pattern `\\s+` matches one or more whitespace characters. The `strip()` method is then used to remove leading and trailing spaces from the resulting text.\n\n4. Finally, the function returns the preprocessed and cleaned text.\n\nIn summary, the `preprocess_v2` function is designed to normalize Unicode characters, remove certain special characters while preserving alphanumeric characters and spaces, and ensure that consecutive spaces are compressed into a single space. This type of text preprocessing is common in natural language processing tasks to ensure consistent and clean input data.","metadata":{}},{"cell_type":"markdown","source":"# The sns.lineplot function from the Seaborn library","metadata":{}},{"cell_type":"markdown","source":"The `sns.lineplot` function from the Seaborn library calculates the average by default when you provide data in a long-form format. In a long-form dataset, each row represents a single observation, and there are columns indicating the grouping variables (in this case, \"DocCatLabels\") and the variable of interest (in this case, \"NOUN\", \"ADP\", \"PUNCT\", \"VERB\").\n\nHere's how Seaborn calculates the average for each line:\n\n1. **Grouping by the x-axis variable:**\n   - Seaborn first groups the data by the values in the \"DocCatLabels\" column.\n   - For each unique value in \"DocCatLabels,\" it considers all the corresponding values in the \"NOUN\", \"ADP\", \"PUNCT\", and \"VERB\" columns.\n\n2. **Calculating the average:**\n   - For each group (each \"DocCatLabels\" value), it calculates the average of the values in the \"NOUN\", \"ADP\", \"PUNCT\", and \"VERB\" columns separately.\n   - The result is a set of average values for each category (\"Noun\", \"Adposition\", \"Punctuation\", \"Verb\") within each \"DocCatLabels\" group.\n\n3. **Plotting the lines:**\n   - Seaborn then plots these average values on the y-axis, using the \"NOUN\", \"ADP\", \"PUNCT\", and \"VERB\" columns as the y-values.\n   - Each line represents the trend of the average values for the corresponding part-of-speech category across different \"DocCatLabels.\"\n\nIn summary, Seaborn's `lineplot` automatically handles the grouping and averaging based on the x-axis variable (\"DocCatLabels\" in your case). If you want to customize the aggregation function or other aspects of the calculation, you can explore Seaborn's documentation for additional parameters.","metadata":{}},{"cell_type":"markdown","source":"### Quality of text","metadata":{}},{"cell_type":"markdown","source":"Checking the quality of text in natural language processing (NLP) problems involves assessing various aspects such as correctness, coherence, clarity, and relevance. Here are some common approaches and considerations for evaluating the quality of text in NLP problems:\n\n1. **Correctness:**\n   - **Grammar and Spelling:** Use language tools or libraries to check for grammatical errors and spelling mistakes. Libraries like NLTK or spaCy often include functionalities for basic language checking.\n   - **Named Entity Recognition (NER):** Evaluate the accuracy of NER models in identifying entities such as names, locations, and organizations.\n\n2. **Coherence and Cohesion:**\n   - **Sentence Structure:** Check the coherence of sentences and the overall flow of the text. Ensure that sentences are logically connected and form a cohesive narrative.\n   - **Coreference Resolution:** Evaluate how well the model resolves references (pronouns, definite noun phrases) to their correct antecedents.\n\n3. **Clarity:**\n   - **Readability Scores:** Use readability formulas (e.g., Flesch-Kincaid, Gunning Fog Index) to assess the readability of the text. Clear, concise writing is often more effective.\n   - **Ambiguity:** Identify and address ambiguous phrases or statements that could be interpreted in multiple ways.\n\n4. **Relevance:**\n   - **Topic Modeling:** Use topic modeling techniques to ensure that the text is focused on relevant topics. This is especially important for extracting information or summarization tasks.\n   - **Sentiment Analysis:** Evaluate sentiment analysis to determine the overall sentiment expressed in the text and whether it aligns with expectations.\n\n5. **Domain-Specific Evaluation:**\n   - **Domain-Specific Metrics:** Define and use metrics specific to your domain. For example, in medical NLP, accuracy in identifying medical entities might be crucial.\n   - **Expert Review:** Have domain experts review the text for accuracy and relevance.\n\n6. **Data Augmentation and Noise:**\n   - **Synthetic Data Testing:** If you've used data augmentation techniques, check whether the model can handle variations introduced during augmentation without sacrificing quality.\n   - **Noise Handling:** Evaluate the model's ability to handle noisy or misspelled text, as real-world data often contains errors.\n\n7. **Evaluation Metrics:**\n   - **Bleu Score (for Machine Translation):** Assess the quality of translated text compared to reference translations.\n   - **ROUGE Score (for Text Summarization):** Measure the overlap between the model-generated summaries and reference summaries.\n\n8. **User Feedback:**\n   - **User Studies:** Collect feedback from end-users to assess the perceived quality and usability of the generated text.\n   - **Iterative Improvement:** Use user feedback for iterative model improvement.\n\nRemember that the evaluation metrics and approaches can vary depending on the specific NLP task you are working on. It's essential to tailor the evaluation to the goals and requirements of your particular application.","metadata":{}},{"cell_type":"markdown","source":"# decontraction, lemmatization, and Named Entity Recognition (NER) ","metadata":{}},{"cell_type":"markdown","source":"\n\n### 1. Decontraction:\n\nDecontraction involves expanding contractions in text. Contractions are shortened versions of words or phrases, such as \"don't\" for \"do not.\" Decontraction aims to convert these contractions into their full forms.\n\n```python\nimport re\n\ndef decontract(text):\n    # Specific cases\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"can not\", text)\n\n    # General cases\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"'ll\", \" will\", text)\n    text = re.sub(r\"'d\", \" would\", text)\n    text = re.sub(r\"'re\", \" are\", text)\n    text = re.sub(r\"'s\", \" is\", text)\n    text = re.sub(r\"'m\", \" am\", text)\n    return text\n\n# Example usage:\ntext_with_contractions = \"I can't believe he won't come. It's gonna be awesome!\"\ntext_decontracted = decontract(text_with_contractions)\nprint(\"Decontracted Text:\", text_decontracted)\n```\n\n**Output:**\n```\nDecontracted Text: I can not believe he will not come. It is gonna be awesome!\n```\n\n### 2. Lemmatization:\n\nLemmatization involves reducing words to their base or root form, considering the word's meaning. This helps in grouping together different forms of the same word.\n\n```python\nimport spacy\n\n# Load spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef lemmatize(text):\n    doc = nlp(text)\n    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n    return lemmatized_text\n\n# Example usage:\ntext_to_lemmatize = \"The cats are running and the dogs are barking.\"\ntext_lemmatized = lemmatize(text_to_lemmatize)\nprint(\"Lemmatized Text:\", text_lemmatized)\n```\n\n**Output:**\n```\nLemmatized Text: the cat be run and the dog be bark .\n```\n\n### 3. Named Entity Recognition (NER):\n\nNER involves identifying and classifying named entities (e.g., persons, organizations, locations) in text.\n\n\n\n```python\nimport spacy\n\n# Load spaCy English model\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef ner(text):\n    doc = nlp(text)\n    entities = [(ent.text, ent.label_) for ent in doc.ents]\n    return entities\n\n# Example usage:\ntext_with_entities = \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\"\nentities_detected = ner(text_with_entities)\nprint(\"Named Entities:\", entities_detected)\n```\n\n**Output:**\n```\nNamed Entities: [('Apple Inc.', 'ORG'), ('Steve Jobs', 'PERSON'), ('Cupertino', 'GPE'), ('California', 'GPE')]\n```\n\nThese outputs demonstrate the results of decontraction, lemmatization, and NER for the provided examples. Keep in mind that the quality of NER results can vary based on the complexity and context of the text.\n\nThese are basic examples, and you can further customize or extend these functions based on your specific requirements. Adjustments might be needed depending on the peculiarities of your text data and the language you are working with.","metadata":{}}]}